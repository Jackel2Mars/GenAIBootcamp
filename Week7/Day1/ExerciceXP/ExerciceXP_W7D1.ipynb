{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee5ea33",
   "metadata": {},
   "source": [
    "Exercise 1: Traditional Vs. Modern NLP: A Comparative Analysis\n",
    "\n",
    "1. Create a table comparing and contrasting the traditional and modern NLP paradigms. Include the following aspects:\n",
    "\n",
    "Feature Engineering (manual vs. automatic)\n",
    "Word Representations (static vs. contextual)\n",
    "Model Architectures (shallow vs. deep)\n",
    "Training Methodology (task-specific vs. pre-training/fine-tuning)\n",
    "Key Examples of Models (e.g., Na√Øve Bayes, BERT)\n",
    "Advantages and Disadvantages of each paradigm.\n",
    "2. Discuss how the evolution from traditional to modern NLP has impacted the scalability and efficiency of NLP applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7be8f",
   "metadata": {},
   "source": [
    "| **Aspect**                 | **Traditional NLP**                            | **Modern NLP**                                     |\n",
    "| -------------------------- | ---------------------------------------------- | -------------------------------------------------- |\n",
    "| **Feature Engineering**    | Manual (handcrafted rules, syntactic features) | Automatic (learned from data)                      |\n",
    "| **Word Representations**   | Static (e.g., one-hot, TF-IDF, Word2Vec)       | Contextual (e.g., BERT embeddings)                 |\n",
    "| **Model Architectures**    | Shallow (Na√Øve Bayes, SVMs, CRFs)              | Deep (Transformers, LSTMs)                         |\n",
    "| **Training Methodology**   | Task-specific                                  | Pre-training + Fine-tuning                         |\n",
    "| **Key Examples of Models** | Na√Øve Bayes, HMMs, SVM                         | BERT, GPT, T5                                      |\n",
    "| **Advantages**             | Interpretable, lightweight                     | High accuracy, generalization, no feature crafting |\n",
    "| **Disadvantages**          | Labor-intensive, poor generalization           | Resource-hungry, less interpretable                |\n",
    "\n",
    "Discussion:\n",
    "Modern NLP enables scalable applications via pre-trained models that adapt to many tasks with minimal data. Traditional methods struggled with vocabulary variation and required domain-specific customization, limiting scalability and consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ee5dc0",
   "metadata": {},
   "source": [
    "üåü Exercise 2: LLM Architecture And Application Scenarios\n",
    "\n",
    "For each of the following LLM architectures (BERT, GPT, T5), describe:\n",
    "\n",
    "The core architectural differences (e.g., bidirectional vs. unidirectional, masked language modeling vs. causal language modeling).\n",
    "A specific real-world application where that architecture excels.\n",
    "Explain why that specific architecture is well suited for that particular application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83684d8c",
   "metadata": {},
   "source": [
    "| **Model** | **Core Architecture**                              | **Best Application**              | **Why It Excels**                    |\n",
    "| --------- | -------------------------------------------------- | --------------------------------- | ------------------------------------ |\n",
    "| **BERT**  | Bidirectional; uses Masked Language Modeling (MLM) | Sentiment analysis/classification | Captures both left and right context |\n",
    "| **GPT**   | Unidirectional; Causal Language Modeling (CLM)     | Text generation and chatbots      | Fluent, forward-focused generation   |\n",
    "| **T5**    | Encoder-Decoder; Text-to-text format               | Translation, summarization        | Flexible for many generation tasks   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18dae8",
   "metadata": {},
   "source": [
    "üåü Exercise 3: The Benefits And Ethical Considerations Of Pre-Training\n",
    "\n",
    "Explain in your own words the five key benefits of pre-trained models discussed in the lesson (improved generalization, reduced need for labeled data, faster fine-tuning, transfer learning, and robustness).\n",
    "Discuss potential ethical concerns associated with pre-training LLMs on massive datasets, such as bias, misinformation, and misuse.\n",
    "Propose potential mitigation strategies to address these ethical concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49715395",
   "metadata": {},
   "source": [
    "‚úÖ Five Key Benefits:\n",
    "Improved Generalization: Pre-trained models capture diverse language patterns.\n",
    "Reduced Need for Labeled Data: Useful in low-resource settings.\n",
    "Faster Fine-Tuning: Fewer epochs and training time needed.\n",
    "Transfer Learning: Apply the same model to different tasks.\n",
    "Robustness: Models learn diverse patterns, reducing overfitting.\n",
    "\n",
    "‚ö†Ô∏è Ethical Concerns:\n",
    "Bias: Models inherit societal and cultural biases from training data.\n",
    "Misinformation: Can propagate inaccurate or misleading content.\n",
    "Misuse: LLMs can generate harmful or manipulative outputs.\n",
    "\n",
    "üõ°Ô∏è Mitigation Strategies:\n",
    "Curate diverse, representative training data.\n",
    "Use bias detection tools and audits.\n",
    "Add content filters and safety layers during deployment.\n",
    "Encourage transparency and responsible use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c36011",
   "metadata": {},
   "source": [
    "üåü Exercise 4 : Transformer Architecture Deep Dive\n",
    "\n",
    "Explain Self-Attention and Multi-Head Attention:\n",
    "\n",
    "Describe in detail how the self-attention mechanism works within a Transformer.\n",
    "Explain the purpose and advantages of multi-head attention compared to single-head attention.\n",
    "Provide a concrete example (different from the lesson) of a sentence and illustrate how multi-head attention might process it, focusing on different relationships between words.\n",
    "Pre-training Objectives:\n",
    "\n",
    "Compare and contrast Masked Language Modeling (MLM) and Causal Language Modeling (CLM).\n",
    "Describe a scenario where MLM would be more appropriate and a scenario where CLM would be more appropriate.\n",
    "Explain why early BERT models used Next Sentence Prediction (NSP) and why modern models tend to avoid it.\n",
    "Transformer Model Selection:\n",
    "\n",
    "You are tasked with building the following NLP applications. For each, specify which type of Transformer model (Encoder-only, Decoder-only, or Encoder-Decoder) would be most suitable and justify your choice.\n",
    "A system that analyzes customer reviews to determine if they are positive, negative, or neutral.\n",
    "A chatbot that can generate creative and engaging responses in a conversation.\n",
    "A service that automatically translates technical documents from English to Spanish.\n",
    "Explain the advantages of the chosen model type for each particular task.\n",
    "Positional Encoding:\n",
    "\n",
    "Explain the purpose of positional encoding, and why it is important for the transformer architecture.\n",
    "Give an example of a situation where the lack of positional encoding would cause a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda84b10",
   "metadata": {},
   "source": [
    "‚úÖ Self-Attention:\n",
    "Calculates attention scores between all tokens in a sequence. Each word attends to every other word, enabling global context.\n",
    "\n",
    "‚úÖ Multi-Head Attention:\n",
    "Runs multiple self-attention mechanisms in parallel. Each head focuses on different aspects of the input.\n",
    "\n",
    "Example:\n",
    "\n",
    "Sentence: \"The bank near the river flooded after the storm.\"\n",
    "\n",
    "Head 1 focuses on ‚Äúbank‚Äìriver‚Äù relation (geographic meaning).\n",
    "Head 2 focuses on ‚Äúflooded‚Äìstorm‚Äù (event cause-effect).\n",
    "Head 3 may attend to ‚Äúbank‚Äìflooded‚Äù (semantic disambiguation).\n",
    "\n",
    "üîÑ Pre-training Objectives:\n",
    "| **MLM**                             | **CLM**                        |\n",
    "| ----------------------------------- | ------------------------------ |\n",
    "| Mask random tokens and predict them | Predict next token in sequence |\n",
    "| Used in BERT                        | Used in GPT                    |\n",
    "| Bi-directional context              | Uni-directional context        |\n",
    "\n",
    "MLM better for: Classification, sentence understanding.\n",
    "CLM better for: Generation tasks like storytelling, dialogue.\n",
    "NSP in BERT:\n",
    "\n",
    "Used to learn sentence-pair relationships. Later models dropped NSP due to limited effectiveness; replaced with tasks like Sentence Order Prediction (SOP) or larger batch contrastive learning.\n",
    "\n",
    "üîß Model Selection for Tasks:\n",
    "| **Task**                 | **Model Type**             | **Justification**                                |\n",
    "| ------------------------ | -------------------------- | ------------------------------------------------ |\n",
    "| Sentiment classification | Encoder-only (e.g., BERT)  | Need deep understanding of input, no generation. |\n",
    "| Chatbot                  | Decoder-only (e.g., GPT)   | Generates coherent, creative responses.          |\n",
    "| Translation              | Encoder-Decoder (e.g., T5) | Processes source and generates target sequences. |\n",
    "\n",
    "üìç Positional Encoding:\n",
    "Transformers lack recurrence, so positional encoding adds order info to token embeddings.\n",
    "\n",
    "Without it:\n",
    "\n",
    "\"The cat sat on the mat.\" and \"Mat the sat cat on the.\" would look the same ‚Äî losing meaning entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f572b653",
   "metadata": {},
   "source": [
    "üåü Exercise 5: BERT Variations - Choose Your Detective\n",
    "\n",
    "For each of the following scenarios, identify which BERT variation (RoBERTa, ALBERT, DistilBERT, ELECTRA, XLM-RoBERTa) would be most suitable and explain why:\n",
    "\n",
    "Scenario 1: Real-time sentiment analysis on mobile app with limited resources.\n",
    "Scenario 2: Research on legal documents requiring high accuracy.\n",
    "Scenario 3: Global customer support in multiple languages.\n",
    "Scenario 4: efficient pretraining, and token replacement detection.\n",
    "Scenario 5: efficient NLP in resource-constrained environments.\n",
    "Create a table comparing the key features and trade-offs of each BERT variation discussed in the lesson. Include aspects like:\n",
    "\n",
    "Training data and methods.\n",
    "\n",
    "Model size and efficiency.\n",
    "Specific optimizations and innovations.\n",
    "Ideal use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada9c43",
   "metadata": {},
   "source": [
    "| **Scenario**                                       | **Best Model**  | **Why**                                                       |\n",
    "| -------------------------------------------------- | --------------- | ------------------------------------------------------------- |\n",
    "| Real-time sentiment on mobile                      | **DistilBERT**  | Lightweight, fast inference                                   |\n",
    "| Legal document research                            | **RoBERTa**     | High accuracy with large corpora                              |\n",
    "| Global support in multiple languages               | **XLM-RoBERTa** | Trained on 100+ languages                                     |\n",
    "| Efficient pretraining, token replacement detection | **ELECTRA**     | Pretraining efficiency via replaced token detection           |\n",
    "| Resource-constrained environments                  | **ALBERT**      | Parameter-sharing reduces size without major performance drop |\n",
    "\n",
    "üìä Comparison Table:\n",
    "| Model          | Training Data                        | Size        | Innovations                      | Use Case                    |\n",
    "| -------------- | ------------------------------------ | ----------- | -------------------------------- | --------------------------- |\n",
    "| **RoBERTa**    | More data, no NSP                    | Large       | Dynamic masking                  | High-accuracy tasks         |\n",
    "| **ALBERT**     | Same as BERT                         | Small       | Parameter sharing, factorization | Low-resource environments   |\n",
    "| **DistilBERT** | BERT (distilled)                     | 40% smaller | Knowledge distillation           | Mobile/real-time inference  |\n",
    "| **ELECTRA**    | Discriminator learns replaced tokens | Compact     | Replaced token detection         | Fast, efficient pretraining |\n",
    "| **XLM-R**      | 2TB multilingual data                | Large       | Multilingual training            | Cross-lingual tasks         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaecbf5",
   "metadata": {},
   "source": [
    "üåü Exercise 6: Softmax Temperature - The Randomness Regulator\n",
    "\n",
    "1. Temperature Scenarios: Describe how the output of a language model would differ in the following scenarios:\n",
    "\n",
    "Softmax temperature set to 0.2.\n",
    "Softmax temperature set to 1.5.\n",
    "Softmax temperature set to 1.\n",
    "2. Application Design:\n",
    "\n",
    "You are designing a system that generates personalized bedtime stories for children. Explain how you would use softmax temperature to control the creativity and coherence of the stories.\n",
    "You are building a system that automatically generates summaries of financial reports. Explain how you would use softmax temperature to ensure accuracy and reliability.\n",
    "Temperature and Bias:\n",
    "\n",
    "Discuss how adjusting softmax temperature might affect the potential for bias in a language model‚Äôs output.\n",
    "Give a practical example."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
