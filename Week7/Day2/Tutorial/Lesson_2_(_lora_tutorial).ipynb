{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4SAGeXapyM4"
      },
      "source": [
        "# LoRA Fine-tuning\n",
        "\n",
        "In this example, we'll fine-tune a base ViT model with two different datasets using LoRA. Then, we'll load the base model and dynamically swap both LoRA adapters depending on the task we want to complete.\n",
        "\n",
        "## Why does this matter?\n",
        "\n",
        "A foundation model knows how to do many things, but it's not great at many tasks. We can fine-tune the model to produce specialized models that are very good at solving specific tasks.\n",
        "\n",
        "We'll use LoRA to fine-tune the foundation model and generate many, specialized adapters. We can load these adapters together with a model to dynamically transform its capabilities.\n",
        "\n",
        "When loading the model, we'll take the foundation model's original weights and apply the LoRA weight changes to it to get the fine-tuned model weights.\n",
        "\n",
        "The beauty of LoRA is that we don't need to fine-tune the entire matrix of weights. Instead, we can get away by fine-tuning two matrices of lower rank. These matrices, when multiplied together, will get us the weight updates we'll need to apply the foundation model to modify its capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQtuHwn3Vy5u"
      },
      "source": [
        "Below are the **steps** we will follow through this tutorial to go from raw image datasets all the way to deploying lightweight, LoRA-fine-tuned Vision Transformer adapters for inference:\n",
        "- 1. **Setup & Utilities**: Install necessary libraries, define helper functions for model sizing, parameter reporting, dataset splitting, and label-ID mappings.\n",
        "- 2. **Data Preparation**: Load and preprocess the Food101 and Cats vs Dogs datasets, split into train/test, and build label2id/id2label mappings so the model knows how to translate between strings and indices.\n",
        "- 3. **Fine-Tuning the Model : Model & LoRA Configuration**: Load the pretrained ViT base model, wrap it with a LoRA adapter configuration (specifying low-rank update matrices), and inspect trainable vs. total parameters.\n",
        "- 4. **Fine-Tuning Loop**: Use a single Trainer loop that, for each dataset configuration, applies our TrainingArguments, trains the LoRA-augmented model, evaluates validation accuracy each epoch, and saves the best adapter.\n",
        "- 5. **Deployment & Running Inference**: Load each saved LoRA adapter on top of the base ViT, retrieve the matching image processor, and run new images through the predict function to obtain human-readable class labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9CIt2LrV6Vu"
      },
      "source": [
        "## 1. Setup & Utilities\n",
        "\n",
        "We will download the following packages :\n",
        "- `transformers` : a library of state-of-the-art pretrained models (BERT, GPT, T5, Vision Transformers, etc.) and high-level pipelines. We use it to load or define models for fine-tuning or inference. To learn more about it, click [here](https://huggingface.co/docs/transformers/en/quicktour).\n",
        "- `datasets` : a fast, memory-mapped library for loading and processing datasets at scale. We use it to download, preprocess (e.g. tokenization), and batch data for training or evaluation.To learn more about it, click [here](https://huggingface.co/docs/datasets/en/installation).\n",
        "- `evaluate` : a lightweight toolkit for computing evaluation metrics like accuracy, F1, BLEU, ROUGE, perplexity. It is very easy to integrate with the HuggingFace Trainer or custom loops. We use it to compute validation/test metrics to monitor performance.\n",
        "- `accelerate` : a thin wrapper to simplify multi-GPU/multi-node training and mixed-precision. It allows us to launch our model in one line `accelerate launch train.py`. We use it to scale training from a single GPU to many GPUs with minimal code changes.\n",
        "- `peft` : stands for Parameter-Efficient Fine-Tuning. It is a techniques for fine-tuning large models using only a small number of additional parameters. LoRA is one of the core PEFT methods. We use this library to adapt huge models on your data without full fine-tuning, reducing memory & compute costs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "adotjRk7pyM5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --quiet transformers accelerate evaluate datasets peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE-F04x1pyM5"
      },
      "source": [
        "We are going to use a **Vision Transformer (ViT) model** pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. ViT-Base-Patch16-224 is the ‚Äúbase‚Äù variant of the Vision Transformer that divides a 224√ó224 input image into non-overlapping 16√ó16 patches, projects each patch into a 768-dimensional embedding, and processes the resulting 14√ó14 sequence with a 12-layer transformer. It was pretrained on ImageNet-1k to show that pure self-attention architectures can match or outperform convolutional models on image classification. You can learn more about it in the [model card](https://huggingface.co/google/vit-base-patch16-224).\n",
        "This model has a size of 346 MB on disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d1k8FV6ppyM6"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"google/vit-base-patch16-224-in21k\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJhXxhD8pyM6"
      },
      "source": [
        "### Creating A Couple Of Helpful Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFp73j7VWfcd"
      },
      "source": [
        "To make our Python notebook cleaner, we will create what we call ‚Äúhelper functions‚Äù to help us abstract away repetitive tasks‚Äîsuch as :\n",
        "- measuring model size with `print_model_size(path)`\n",
        "- reporting trainable parameters with `print_trainable_parameters(model, label)`\n",
        "- splitting datasets with `split_dataset(dataset)`\n",
        "- generating label mappings with `create_label_mappings(dataset)`\n",
        "So that our main training and evaluation code stays concise and easy to read.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rh2WResVpyM6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/user/GenAIBootcamp/Week7/Day2/Tutorial/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from peft import PeftModel, LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "\n",
        "def print_model_size(path):\n",
        "    size = 0\n",
        "    for f in os.scandir(path):\n",
        "        size += os.path.getsize(f)\n",
        "\n",
        "    print(f\"Model size: {(size / 1e6):.2} MB\")\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model, label):\n",
        "    parameters, trainable = 0, 0\n",
        "\n",
        "    for _, p in model.named_parameters():\n",
        "        parameters += p.numel()\n",
        "        trainable += p.numel() if p.requires_grad else 0\n",
        "\n",
        "    print(f\"{label} trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")\n",
        "\n",
        "\n",
        "def split_dataset(dataset):\n",
        "    dataset_splits = dataset.train_test_split(test_size=0.1)\n",
        "    return dataset_splits.values()\n",
        "\n",
        "\n",
        "def create_label_mappings(dataset):\n",
        "    label2id, id2label = dict(), dict()\n",
        "    for i, label in enumerate(dataset.features[\"label\"].names):\n",
        "        label2id[label] = i\n",
        "        id2label[i] = label\n",
        "\n",
        "    return label2id, id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovf43z7mpyM7"
      },
      "source": [
        "## Loading and Preparing the Datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW-mWHYZWlNf"
      },
      "source": [
        "We'll be loading two different datasets to fine-tune the base model using the `datasets` package:\n",
        "- A dataset of pictures of food `food101`, more info [here](\n",
        "- A dataset of pictures of cats and dogs `microsoft/cats_vs_dogs`, more info [here](\n",
        "\n",
        "Before running the following code, you need to have your HuggingFace token saved in secret key. If you dont, here's how to create one :\n",
        "Google Colab has a built-in ‚ÄúSecrets‚Äù pane that lets you store API keys (or any other secret) so they never get hard-coded into your notebook. Here‚Äôs how to add your Hugging Face token and consume it in your code.\n",
        "\n",
        "To add your token to Colab‚Äôs Secrets\n",
        "1. Open your Colab notebook.  \n",
        "2. Click the **üîë Secrets** icon on the left sidebar.  \n",
        "3. In the Secrets panel that appears, click **+ Add a secret**.  \n",
        "4. In the **Key** field enter \"HUGGINGFACE_TOKEN\"\n",
        "5. In the **Value** field paste your Hugging Face token (the string you copied from your Hugging Face profile, if you dont have you can create a new one following this [tutorial](https://www.geeksforgeeks.org/how-to-access-huggingface-api-key/)).  \n",
        "6. Click **Save**.  \n",
        "7. Close the Secrets pane.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KeEZ6rLSpyM7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /Users/user/.cache/huggingface/datasets/ethz___food101/default/0.0.0/e06acf2a88084f04bce4d4a525165d68e0a36c38\n",
            "Found cached dataset food101 (/Users/user/.cache/huggingface/datasets/ethz___food101/default/0.0.0/e06acf2a88084f04bce4d4a525165d68e0a36c38)\n",
            "Loading Dataset info from /Users/user/.cache/huggingface/datasets/ethz___food101/default/0.0.0/e06acf2a88084f04bce4d4a525165d68e0a36c38\n",
            "Constructing Dataset for split train[:10000], from /Users/user/.cache/huggingface/datasets/ethz___food101/default/0.0.0/e06acf2a88084f04bce4d4a525165d68e0a36c38\n",
            "`trust_remote_code` is not supported anymore.\n",
            "Please check that the Hugging Face dataset 'microsoft/cats_vs_dogs' isn't based on a loading script and remove `trust_remote_code`.\n",
            "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /Users/user/.cache/huggingface/datasets/microsoft___cats_vs_dogs/default/0.0.0/b5ae3589204019bc2cc97e99e4914a54589333ef\n",
            "Found cached dataset cats_vs_dogs (/Users/user/.cache/huggingface/datasets/microsoft___cats_vs_dogs/default/0.0.0/b5ae3589204019bc2cc97e99e4914a54589333ef)\n",
            "Loading Dataset info from /Users/user/.cache/huggingface/datasets/microsoft___cats_vs_dogs/default/0.0.0/b5ae3589204019bc2cc97e99e4914a54589333ef\n",
            "Constructing Dataset for split train, from /Users/user/.cache/huggingface/datasets/microsoft___cats_vs_dogs/default/0.0.0/b5ae3589204019bc2cc97e99e4914a54589333ef\n",
            "Caching indices mapping at /Users/user/.cache/huggingface/datasets/ethz___food101/default/0.0.0/e06acf2a88084f04bce4d4a525165d68e0a36c38/cache-62652d392bdaa9ba.arrow\n",
            "Done writing 9000 indices in 72000 bytes /Users/user/.cache/huggingface/datasets/ethz___food101/default/0.0.0/e06acf2a88084f04bce4d4a525165d68e0a36c38/tmp3ip8im_w.\n",
            "Caching indices mapping at /Users/user/.cache/huggingface/datasets/ethz___food101/default/0.0.0/e06acf2a88084f04bce4d4a525165d68e0a36c38/cache-66ac52ca2bb1d0fc.arrow\n",
            "Done writing 1000 indices in 8000 bytes /Users/user/.cache/huggingface/datasets/ethz___food101/default/0.0.0/e06acf2a88084f04bce4d4a525165d68e0a36c38/tmpmqjmrlin.\n",
            "Caching indices mapping at /Users/user/.cache/huggingface/datasets/microsoft___cats_vs_dogs/default/0.0.0/b5ae3589204019bc2cc97e99e4914a54589333ef/cache-c1fb7605d52c7c49.arrow\n",
            "Done writing 21069 indices in 168552 bytes /Users/user/.cache/huggingface/datasets/microsoft___cats_vs_dogs/default/0.0.0/b5ae3589204019bc2cc97e99e4914a54589333ef/tmp2alk56q_.\n",
            "Caching indices mapping at /Users/user/.cache/huggingface/datasets/microsoft___cats_vs_dogs/default/0.0.0/b5ae3589204019bc2cc97e99e4914a54589333ef/cache-3747ba6f4a7841d3.arrow\n",
            "Done writing 2341 indices in 18728 bytes /Users/user/.cache/huggingface/datasets/microsoft___cats_vs_dogs/default/0.0.0/b5ae3589204019bc2cc97e99e4914a54589333ef/tmpxnf6y7f1.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "from datasets import logging\n",
        "logging.set_verbosity_debug()\n",
        "\n",
        "# This is the food dataset\n",
        "dataset1 = load_dataset(\"ethz/food101\", split=\"train[:10000]\") # tells load_dataset to load only the first 10,000 examples from the ‚Äútrain‚Äù split of the Food101 dataset\n",
        "\n",
        "# This is the datasets of pictures of cats and dogs.\n",
        "# Notice we need to rename the label column so we can\n",
        "# reuse the same code for both datasets.\n",
        "dataset2 = load_dataset(\"microsoft/cats_vs_dogs\", split=\"train\", trust_remote_code=True)\n",
        "dataset2 = dataset2.rename_column(\"labels\", \"label\")\n",
        "\n",
        "dataset1_train, dataset1_test = split_dataset(dataset1)\n",
        "dataset2_train, dataset2_test = split_dataset(dataset2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWi26oF-pyM7"
      },
      "source": [
        "We need these mappings to properly fine-tune the Vision Transformer model. You can find more information in the [`PretrainedConfig`](https://huggingface.co/docs/transformers/en/main_classes/configuration#transformers.PretrainedConfig) documentation, under the \"Parameters for fine-tuning tasks\" section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c4S9KwlppyM8"
      },
      "outputs": [],
      "source": [
        "dataset1_label2id, dataset1_id2label = create_label_mappings(dataset1)\n",
        "dataset2_label2id, dataset2_id2label = create_label_mappings(dataset2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvz8PamhWvnX"
      },
      "source": [
        "You then pass these into your model‚Äôs configuration (via PretrainedConfig) so that:\n",
        "- During training, when you see a sample labeled \"dog\", you can look up label2id[\"dog\"] ‚Üí 1 and compute the loss against class index 1.\n",
        "- During evaluation or inference, the model will predict a class index (say, 2) and you can convert that back to the human‚Äêreadable label \"elephant\" via id2label[2].\n",
        "\n",
        "Without these mappings, the model wouldn‚Äôt know how to translate between the string labels in your data and the integer IDs it actually trains on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cHKdprWopyM8"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"model1\": {\n",
        "        \"train_data\": dataset1_train,\n",
        "        \"test_data\": dataset1_test,\n",
        "        \"label2id\": dataset1_label2id,\n",
        "        \"id2label\": dataset1_id2label,\n",
        "        \"epochs\": 5,\n",
        "        \"path\": \"./lora-model1\"\n",
        "    },\n",
        "    \"model2\": {\n",
        "        \"train_data\": dataset2_train,\n",
        "        \"test_data\": dataset2_test,\n",
        "        \"label2id\": dataset2_label2id,\n",
        "        \"id2label\": dataset2_id2label,\n",
        "        \"epochs\": 1,\n",
        "        \"path\": \"./lora-model2\"\n",
        "    },\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGdS25aKpyM8"
      },
      "source": [
        "config is simply a Python dictionary that acts as a centralized registry of all the settings and data you need for each of your training runs. Rather than hard-coding values all over your script, you bundle them together under human-readable keys (here, \"model1\" and \"model2\"). Each entry contains:\n",
        "- train_data / test_data: the dataset splits you‚Äôll use to teach and evaluate the model\n",
        "- label2id / id2label: the mappings between your string class names and the integer IDs the model actually learns\n",
        "- epochs: how many full passes over the training set you want for that particular experiment\n",
        "- path: the filesystem location where you‚Äôll save the fine-tuned (LoRA-augmented) model\n",
        "\n",
        "By organizing everything into one config dict, you can write a single loop that:\n",
        "- Picks up the right datasets and mappings for each model\n",
        "- Reads the correct number of epochs\n",
        "- Saves each experiment‚Äôs outputs in its own folder\n",
        "\n",
        "This pattern keeps your code DRY (Don‚Äôt Repeat Yourself), makes it easy to add new experiments (just add another key to the dict), and makes your workflow more transparent and reproducible.\n",
        "\n",
        "We want to automatically load the model‚Äôs own image processor so our pictures get resized, normalized, and turned into tensors exactly the way the ViT expects‚Äîsaving us from having to write and debug all that preprocessing by hand. Let's create an image processor automatically from the [preprocessor configuration](https://huggingface.co/google/vit-base-patch16-224/blob/main/preprocessor_config.json) specified by the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "umlWkRm3pyM8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqWZMSzRpyM8"
      },
      "source": [
        "We can now prepare the preprocessing pipeline to transform the images in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LpysvLRzpyM8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Set __getitem__(key) output type to custom for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Set __getitem__(key) output type to custom for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Set __getitem__(key) output type to custom for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "Set __getitem__(key) output type to custom for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n"
          ]
        }
      ],
      "source": [
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "preprocess_pipeline = Compose([\n",
        "    Resize(image_processor.size[\"height\"]),\n",
        "    CenterCrop(image_processor.size[\"height\"]),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
        "])\n",
        "\n",
        "def preprocess(batch):\n",
        "    batch[\"pixel_values\"] = [\n",
        "        preprocess_pipeline(image.convert(\"RGB\")) for image in batch[\"image\"]\n",
        "    ]\n",
        "    return batch\n",
        "\n",
        "\n",
        "# Let's set the transform function to every train and test sets\n",
        "for cfg in config.values():\n",
        "    cfg[\"train_data\"].set_transform(preprocess)\n",
        "    cfg[\"test_data\"].set_transform(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIq1httYW78J"
      },
      "source": [
        "Now that is is done, we will fine tune the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DxckFC8pyM8"
      },
      "source": [
        "## Fine-Tuning the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGBGSwdNW7Im"
      },
      "source": [
        "These are functions that we'll need to fine-tune the model.\n",
        "- `data_collate(examples)` : This function takes a list of individual examples (each containing \"pixel_values\" and \"label\") and stacks them into batched tensors: a pixel_values tensor of shape (batch_size, C, H, W) and a labels tensor of shape (batch_size,). Its purpose is to produce the correctly formatted input dict that the Hugging Face Trainer expects for both training and evaluation.\n",
        "- `compute_metrics(eval_pred)` : Given an EvalPrediction object with raw model logits and true label IDs, it selects the highest‚Äêscoring class via argmax and then computes accuracy against the references using the evaluate library. You plug this function into your Trainer so that it reports accuracy automatically at each evaluation step.\n",
        "- `get_base_model(label2id, id2label)` : This function loads the pretrained Vision Transformer classifier from the specified checkpoint, injecting your dataset‚Äôs label2id and id2label mappings and allowing mismatched head sizes to be resized. It gives you a ready‚Äêto‚Äêuse base model configured for your specific classification labels.\n",
        "- `build_lora_model(label2id, id2label)` : First, it calls get_base_model to instantiate the ViT classifier and prints how many parameters would be trainable if you fine‚Äêtuned all of them. Then it creates a LoraConfig (specifying low‚Äêrank projection size, dropout, target modules, etc.), wraps the base model with LoRA adapters via get_peft_model, prints the new trainable‚Äêparameter count (just the adapters), and returns the lightweight LoRA‚Äêaugmented model ready for efficient fine‚Äêtuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d2Sy5ergpyM8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "import torch\n",
        "from peft import PeftModel, LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def data_collate(examples):\n",
        "    \"\"\"\n",
        "    Prepare a batch of examples from a list of elements of the\n",
        "    train or test datasets.\n",
        "    \"\"\"\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute the model's accuracy on a batch of predictions.\n",
        "    \"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "\n",
        "\n",
        "def get_base_model(label2id, id2label):\n",
        "    \"\"\"\n",
        "    Create an image classification base model from\n",
        "    the model checkpoint.\n",
        "    \"\"\"\n",
        "    return AutoModelForImageClassification.from_pretrained(\n",
        "        model_checkpoint,\n",
        "        label2id=label2id,\n",
        "        id2label=id2label,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def build_lora_model(label2id, id2label):\n",
        "    \"\"\"Build the LoRA model to fine-tune the base model.\"\"\"\n",
        "    model = get_base_model(label2id, id2label)\n",
        "    print_trainable_parameters(model, label=\"Base model\")\n",
        "\n",
        "    config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"query\", \"value\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        modules_to_save=[\"classifier\"],\n",
        "    )\n",
        "\n",
        "    lora_model = get_peft_model(model, config)\n",
        "    print_trainable_parameters(lora_model, label=\"LoRA\")\n",
        "\n",
        "    return lora_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmck-YTOpyM9"
      },
      "source": [
        "Let's now configure the fine-tuning process. Before kicking off training, we create a `TrainingArguments` object that tells the Hugging Face `Trainer` how, when, and with what settings to run our fine-tuning job:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vHojD4vupyM9"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 32\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./model-checkpoints\", #Directory where checkpoints and the final model are saved.\n",
        "    remove_unused_columns=False, #Keeps all dataset columns (e.g. \"pixel_values\" and \"labels\"), even if some aren‚Äôt used by the model‚Äôs forward().\n",
        "    eval_strategy=\"epoch\", # Runs evaluation once at the end of every epoch.\n",
        "    save_strategy=\"epoch\", # Saves a model checkpoint after each epoch finishes.\n",
        "    learning_rate=5e-3, # Initial optimizer learning rate‚Äîhigher than usual since LoRA adapters converge quickly.\n",
        "    per_device_train_batch_size=batch_size, # Number of training examples per device (GPU/CPU) per step (128 here).\n",
        "    per_device_eval_batch_size=batch_size, # Number of evaluation examples per device per step (128 here).\n",
        "    gradient_accumulation_steps=4, # Accumulates gradients over 4 forward/backward passes before updating‚Äîsimulating a batch size of 512 without extra memory.\n",
        "    fp16=False, # Enables mixed-precision (half-precision) training to speed up computation and reduce memory usage.\n",
        "    logging_steps=10, # Logs training loss and throughput metrics every 10 steps.\n",
        "    load_best_model_at_end=True, # After training, automatically reloads the checkpoint that achieved the best validation metric.\n",
        "    metric_for_best_model=\"accuracy\", # Uses validation accuracy to decide which checkpoint is ‚Äúbest.‚Äù\n",
        "    label_names=[\"labels\"], # Tells the trainer which field(s) in your batch dictionary contain the ground-truth labels.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLzljP5TpyM9"
      },
      "source": [
        "These settings give you:\n",
        "- Regular evaluation and checkpointing to monitor progress.\n",
        "- Memory-efficient training (mixed precision + gradient accumulation).\n",
        "- Automatic best-model selection based on validation accuracy.\n",
        "\n",
        "Let's now fine-tune both models. We loop over our `config` dict to train each LoRA-augmented ViT on its respective dataset, evaluate its performance, and save the adapter.\n",
        "We need now to automate the process: ‚Äúfor each model configuration ‚Üí build the model ‚Üí train it ‚Üí evaluate its performance ‚Üí save the lightweight adapter and report its size.‚Äù That way you get two fully fine-tuned, evaluated, and saved adapters with just a few lines of code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fp16 enabled? False\n"
          ]
        }
      ],
      "source": [
        "print(\"fp16 enabled?\", training_arguments.fp16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "WvbTuggCpyM9",
        "outputId": "4ccd27e3-c129-4fb4-b323-08e3fa77b390"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/var/folders/9m/2x6dfbbn0cq5m_g98d1wpw640000gn/T/ipykernel_31211/3484335089.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model trainable parameters: 85,876,325/85,876,325 (100.00%)\n",
            "LoRA trainable parameters: 667,493/86,543,818 (0.77%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/user/GenAIBootcamp/Week7/Day2/Tutorial/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='355' max='355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [355/355 28:07, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.224800</td>\n",
              "      <td>0.183152</td>\n",
              "      <td>0.942000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.101700</td>\n",
              "      <td>0.194382</td>\n",
              "      <td>0.940000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.034400</td>\n",
              "      <td>0.181412</td>\n",
              "      <td>0.941000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.175343</td>\n",
              "      <td>0.947000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.004700</td>\n",
              "      <td>0.178264</td>\n",
              "      <td>0.950000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done writing 1000 examples in 8000 bytes /Users/user/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "/Users/user/GenAIBootcamp/Week7/Day2/Tutorial/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Done writing 1000 examples in 8000 bytes /Users/user/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "/Users/user/GenAIBootcamp/Week7/Day2/Tutorial/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Done writing 1000 examples in 8000 bytes /Users/user/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "/Users/user/GenAIBootcamp/Week7/Day2/Tutorial/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Done writing 1000 examples in 8000 bytes /Users/user/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "/Users/user/GenAIBootcamp/Week7/Day2/Tutorial/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Done writing 1000 examples in 8000 bytes /Users/user/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "/Users/user/GenAIBootcamp/Week7/Day2/Tutorial/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 00:16]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done writing 1000 examples in 8000 bytes /Users/user/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation accuracy: 0.95\n",
            "Model size: 2.7 MB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/var/folders/9m/2x6dfbbn0cq5m_g98d1wpw640000gn/T/ipykernel_31211/3484335089.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model trainable parameters: 85,800,194/85,800,194 (100.00%)\n",
            "LoRA trainable parameters: 591,362/86,391,556 (0.68%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/user/GenAIBootcamp/Week7/Day2/Tutorial/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='165' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [165/165 12:53, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.013100</td>\n",
              "      <td>0.009254</td>\n",
              "      <td>0.997010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done writing 2341 examples in 18728 bytes /Users/user/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n",
            "/Users/user/GenAIBootcamp/Week7/Day2/Tutorial/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [74/74 00:38]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done writing 2341 examples in 18728 bytes /Users/user/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow.\n",
            "Set __getitem__(key) output type to python objects for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation accuracy: 0.9970098248611704\n",
            "Model size: 2.4 MB\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "for cfg in config.values(): # Iterates through each model‚Äôs settings (model1, then model2).\n",
        "    # 1. Set the number of epochs for this experiment\n",
        "    training_arguments.num_train_epochs = cfg[\"epochs\"] # Dynamically updates the .num_train_epochs field so each model trains for its specified number of epochs.\n",
        "\n",
        "    # 2. Instantiate the Trainer\n",
        "    trainer = Trainer(\n",
        "        build_lora_model(cfg[\"label2id\"], cfg[\"id2label\"]), # Builds a fresh LoRA-wrapped ViT using build_lora_model(...).\n",
        "        training_arguments, # Supplies our TrainingArguments (checkpointing, mixed precision, etc.).\n",
        "        train_dataset=cfg[\"train_data\"], # Pass in the preprocessed HF Datasets.\n",
        "        eval_dataset=cfg[\"test_data\"], # Pass in the preprocessed HF Datasets.\n",
        "        tokenizer=image_processor, # Here we use the AutoImageProcessor as the collating/tokenizing function.\n",
        "        compute_metrics=compute_metrics, # Hook to compute and log accuracy after each evaluation.\n",
        "        data_collator=data_collate, # Custom collator that packages pixel tensors and labels into batches.\n",
        "    )\n",
        "\n",
        "    # 3. Run training\n",
        "    results = trainer.train() # Executes the training loop, saving checkpoints per epoch as configured.\n",
        "\n",
        "    # 4. Evaluate on the test split\n",
        "    evaluation_results = trainer.evaluate(cfg['test_data']) # Runs inference on the test split and returns metrics like eval_accuracy.\n",
        "    print(f\"Evaluation accuracy: {evaluation_results['eval_accuracy']}\")\n",
        "\n",
        "    # 5. Save the fine-tuned adapter and report its size\n",
        "    # We can now save the fine-tuned model to disk.\n",
        "    trainer.save_model(cfg[\"path\"]) # Exports the LoRA adapter (and base weights) to the specified folder.\n",
        "    print_model_size(cfg[\"path\"]) # Reports the on-disk size of the saved adapter, demonstrating the parameter-efficiency of LoRA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WddGcEL4pyM9"
      },
      "source": [
        "## Running Inference\n",
        "\n",
        "This block defines two functions‚Äî`build_inference_model` and `predict`‚Äîthat together let you load a fine-tuned LoRA adapter on top of the base ViT model and then run it on new images to obtain human-readable class predictions.\n",
        "We will create two functions :  \n",
        "- `build_inference_model`:\n",
        "    - Loads the base ViT classifier configured for your dataset‚Äôs labels.\n",
        "    - Wraps it with the LoRA adapter weights you fine-tuned and saved.\n",
        "\n",
        "- `predict`:\n",
        "    - Preprocesses any new PIL image exactly the way the model expects.\n",
        "    - Runs a forward pass to get predicted logits.\n",
        "    - Chooses the highest-scoring class and returns the human-readable label.\n",
        "\n",
        "Together, these functions let you deploy your fine-tuned LoRA adapters: you simply call build_inference_model(...) once to load the model, then repeatedly call predict(image, model, image_processor) on new images to get quick, accurate classifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TWVM3DKppyM9"
      },
      "outputs": [],
      "source": [
        "def build_inference_model(label2id, id2label, lora_adapter_path):\n",
        "    \"\"\"Build the model that will be used to run inference.\"\"\"\n",
        "    # Load the base Vision Transformer with correct label mappings\n",
        "    model = get_base_model(label2id, id2label)\n",
        "    # Apply the saved LoRA adapter weights on top of the base model\n",
        "    return PeftModel.from_pretrained(model, lora_adapter_path)\n",
        "\n",
        "\n",
        "def predict(image, model, image_processor):\n",
        "    \"\"\"Predict the class represented by the supplied image.\"\"\"\n",
        "    # Convert the input PIL image to RGB and run it through the model‚Äôs image processor,\n",
        "    # returning a PyTorch tensor dictionary (e.g. {\"pixel_values\": ...})\n",
        "    encoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")\n",
        "    # Disable gradient tracking since we‚Äôre only doing a forward pass\n",
        "    with torch.no_grad():\n",
        "        # Forward the processed image through the model to get raw outputs\n",
        "        outputs = model(**encoding)\n",
        "        # Extract the logits (unnormalized class scores)\n",
        "        logits = outputs.logits\n",
        "    # Select the class index with the highest logit score\n",
        "    class_index = logits.argmax(-1).item()\n",
        "    # Convert that index back to the original label string\n",
        "    return model.config.id2label[class_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcH4o-I2pyM-"
      },
      "source": [
        "Now, we need to create two inference models, one using each of the LoRA adapters. Here we loop over our `config` entries to load two separate inference-ready models (one per LoRA adapter) and the matching image processor for each.  This loop prepares everything you need for deployment‚Äîinference-ready models and their matching preprocessors‚Äîso that downstream code can simply reference cfg[\"inference_model\"] and cfg[\"image_processor\"] to run predictions on new images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qN0OoU5pyM-",
        "outputId": "8930fb53-9381-46dd-ad07-57e0c1e01a01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "for cfg in config.values():\n",
        "    # Load the base ViT + the fine-tuned LoRA adapter from disk\n",
        "    cfg[\"inference_model\"] = build_inference_model(\n",
        "        cfg[\"label2id\"],      # mapping from string labels ‚Üí IDs\n",
        "        cfg[\"id2label\"],      # mapping from IDs ‚Üí string labels\n",
        "        cfg[\"path\"]           # path where the LoRA adapter is saved\n",
        "    )\n",
        "\n",
        "    # Load the exact image processor (preprocessor config) saved with that adapter,\n",
        "    # so it uses the same resize/normalize settings as during training\n",
        "    cfg[\"image_processor\"] = AutoImageProcessor.from_pretrained(cfg[\"path\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmENUixHpyM-"
      },
      "source": [
        "Here is a list of sample images and the model that we need to use to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TkzjRXQpyM-"
      },
      "outputs": [],
      "source": [
        "samples = [\n",
        "    {\n",
        "        \"image\": \"https://www.foodandwine.com/thmb/P-wchqf52J0lF5Ko5DIPhK0d8YM=/750x0/filters:no_upscale():max_bytes(150000):strip_icc()/FAW-recipes-pasta-sausage-basil-and-mustard-hero-06-cfd1c0a2989e474ea7e574a38182bbee.jpg\",\n",
        "        \"model\": \"model1\",\n",
        "    },\n",
        "    {\n",
        "        \"image\": \"https://wallpapers.com/images/featured/kitty-cat-pictures-nzlg8fu5sqx1m6qj.jpg\",\n",
        "        \"model\": \"model2\",\n",
        "    },\n",
        "    {\n",
        "        \"image\": \"https://i.natgeofe.com/n/5f35194b-af37-4f45-a14d-60925b280986/NationalGeographic_2731043_3x4.jpg\",\n",
        "        \"model\": \"model2\",\n",
        "    },\n",
        "    {\n",
        "        \"image\": \"https://ichef.bbci.co.uk/food/ic/food_16x9_1600/recipes/quick_flatbreads_43123_16x9.jpg\",\n",
        "        \"model\": \"model1\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlkjXEtHpyM-"
      },
      "source": [
        "We can now run predictions on every sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UORUvxAGpyM-",
        "outputId": "4962bd7e-9fde-4da1-8382-ec02e588a8f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: risotto\n",
            "Prediction: cat\n",
            "Prediction: dog\n",
            "Prediction: pizza\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image         # Import PIL for image loading and manipulation\n",
        "import requests                # Import requests to fetch images from URLs\n",
        "\n",
        "for sample in samples:\n",
        "    # 1. Download the image safely\n",
        "    response = requests.get(sample[\"image\"], headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            image = Image.open(BytesIO(response.content))\n",
        "        except Image.UnidentifiedImageError:\n",
        "            print(f\"Failed to identify image from URL: {sample['image']}\")\n",
        "            continue\n",
        "    else:\n",
        "        print(f\"Failed to fetch image from URL: {sample['image']} (status code: {response.status_code})\")\n",
        "        continue\n",
        "\n",
        "    # 2. Retrieve the correct inference model & processor for this sample\n",
        "    inference_model   = config[sample[\"model\"]][\"inference_model\"]\n",
        "    image_processor   = config[sample[\"model\"]][\"image_processor\"]\n",
        "\n",
        "    # 3. Run the image through our `predict` helper to get a label\n",
        "    prediction = predict(image, inference_model, image_processor)\n",
        "\n",
        "    # 4. Print out the result\n",
        "    print(f\"Prediction: {prediction}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
