{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a4c92b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/GenAIBootcamp/Week7/Day2/DailyChallenge/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples loaded: 251\n",
      "Dataset({\n",
      "    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 5\n",
      "})\n",
      "trainable params: 98,304 || all params: 559,312,896 || trainable%: 0.01757585078102687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/Users/user/GenAIBootcamp/Week7/Day2/DailyChallenge/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1506' max='1506' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1506/1506 18:41, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.859200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.752100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/GenAIBootcamp/Week7/Day2/DailyChallenge/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/user/GenAIBootcamp/Week7/Day2/DailyChallenge/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in logits? False\n",
      "Infs in logits? False\n",
      "Negative values? True\n",
      "['Two things are infinite:  time and space. Time, because we cannot stop living, and space, because we cannot stop travelling. If one of these is not right, then the other is wrong, and the two have nothing to do with each']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load foundation model and tokenizer\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Load dataset (10% sample)\n",
    "data = load_dataset(\"Abirate/english_quotes\", split=\"train[:10%]\")\n",
    "print(f\"Number of samples loaded: {len(data)}\")\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_function(samples):\n",
    "    tokenized= tokenizer(samples[\"quote\"], truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    # Convert to tensor for easier checking\n",
    "    input_ids_tensor = torch.tensor(input_ids)\n",
    "    \n",
    "    if torch.isnan(input_ids_tensor).any():\n",
    "        print(\"NaNs found in tokenized input_ids!\")\n",
    "    if (input_ids_tensor < 0).any():\n",
    "        print(\"Negative values found in tokenized input_ids!\")\n",
    "    else:\n",
    "        print(\"Token IDs look valid (no negatives or NaNs).\")\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "data = data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Show a small sample\n",
    "train_sample = data.select(range(5))\n",
    "print(train_sample)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=1,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Inject LoRA layers into foundation model\n",
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "# Set device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "peft_model.to(device)\n",
    "\n",
    "# Training configuration\n",
    "output_directory = os.path.join(\"../cache/working\", \"peft_lab_outputs\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=output_directory,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate= 5e-5, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=6\n",
    ")\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "import torch\n",
    "\n",
    "class DebugCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        inputs = kwargs.get(\"inputs\")\n",
    "        outputs = kwargs.get(\"outputs\")\n",
    "\n",
    "        if inputs is not None:\n",
    "            input_ids = inputs.get(\"input_ids\")\n",
    "            labels = inputs.get(\"labels\")\n",
    "\n",
    "            if input_ids is not None:\n",
    "                if torch.isnan(input_ids).any():\n",
    "                    print(\"NaNs found in input_ids!\")\n",
    "                if (input_ids < 0).any():\n",
    "                    print(\"Negative values found in input_ids!\")\n",
    "                print(\"Input IDs sample:\", input_ids[0][:10])\n",
    "\n",
    "            if labels is not None:\n",
    "                if torch.isnan(labels).any():\n",
    "                    print(\"NaNs found in labels!\")\n",
    "                if (labels < 0).any():\n",
    "                    print(\"Negative values found in labels!\")\n",
    "                print(\"Labels sample:\", labels[0][:10])\n",
    "\n",
    "        if outputs is not None:\n",
    "            loss = getattr(outputs, \"loss\", None)\n",
    "            logits = getattr(outputs, \"logits\", None)\n",
    "\n",
    "            if loss is not None:\n",
    "                print(f\"Loss at step {state.global_step}: {loss.item()}\")\n",
    "\n",
    "            if logits is not None:\n",
    "                if torch.isnan(logits).any():\n",
    "                    print(\"NaNs found in logits!\")\n",
    "                if torch.isinf(logits).any():\n",
    "                    print(\"Infs found in logits!\")\n",
    "                print(\"Logits shape:\", logits.shape)\n",
    "                print(\"Logits sample:\", logits[0, 0, :5])\n",
    "\n",
    "        #if state.global_step >= 10:\n",
    "        #    control.should_training_stop = True\n",
    "\n",
    "\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=data,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[DebugCallback()]\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "peft_model_path = os.path.join(output_directory, f\"peft_model_{int(time.time())}\")\n",
    "trainer.model.save_pretrained(peft_model_path)\n",
    "\n",
    "# Inference\n",
    "inputs = tokenizer(\"Two things are infinite: \", return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = peft_model(**inputs).logits\n",
    "    print(\"NaNs in logits?\", torch.isnan(logits).any().item())\n",
    "    print(\"Infs in logits?\", torch.isinf(logits).any().item())\n",
    "    print(\"Negative values?\", (logits < 0).any().item())\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = peft_model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
