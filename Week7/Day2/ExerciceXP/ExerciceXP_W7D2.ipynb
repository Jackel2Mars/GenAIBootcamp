{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca77b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,\n",
      "          0.3239, -0.1085]])\n",
      "Linear(in_features=10, out_features=5, bias=True)\n",
      "Original output: tensor([[ 0.4257,  0.0299, -0.1865, -0.3084,  0.4543]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.4257,  0.0299, -0.1865, -0.3084,  0.4543]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 0.4257,  0.0299, -0.1865, -0.3084,  0.4543]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "mps\n",
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Image batch dimensions: torch.Size([64, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([64])\n",
      "Epoch: 001/005 | Batch 000/938 | Loss: 2.3104\n",
      "Epoch: 001/005 | Batch 400/938 | Loss: 0.3209\n",
      "Epoch: 001/005 | Batch 800/938 | Loss: 0.2511\n",
      "Epoch: 001/005 | Training Accuracy: 96.28%\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 002/005 | Batch 000/938 | Loss: 0.1005\n",
      "Epoch: 002/005 | Batch 400/938 | Loss: 0.0729\n",
      "Epoch: 002/005 | Batch 800/938 | Loss: 0.0303\n",
      "Epoch: 002/005 | Training Accuracy: 98.05%\n",
      "Time elapsed: 0.29 min\n",
      "Epoch: 003/005 | Batch 000/938 | Loss: 0.0180\n",
      "Epoch: 003/005 | Batch 400/938 | Loss: 0.1264\n",
      "Epoch: 003/005 | Batch 800/938 | Loss: 0.0407\n",
      "Epoch: 003/005 | Training Accuracy: 98.30%\n",
      "Time elapsed: 0.42 min\n",
      "Epoch: 004/005 | Batch 000/938 | Loss: 0.0556\n",
      "Epoch: 004/005 | Batch 400/938 | Loss: 0.0073\n",
      "Epoch: 004/005 | Batch 800/938 | Loss: 0.0049\n",
      "Epoch: 004/005 | Training Accuracy: 99.03%\n",
      "Time elapsed: 0.55 min\n",
      "Epoch: 005/005 | Batch 000/938 | Loss: 0.0176\n",
      "Epoch: 005/005 | Batch 400/938 | Loss: 0.0250\n",
      "Epoch: 005/005 | Batch 800/938 | Loss: 0.0064\n",
      "Epoch: 005/005 | Training Accuracy: 99.16%\n",
      "Time elapsed: 0.67 min\n",
      "Total Training Time: 0.67 min\n",
      "Test accuracy: 97.82%\n",
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=784, out_features=256, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): LinearWithLoRAMerged(\n",
      "      (linear): Linear(in_features=128, out_features=10, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Test accuracy orig model: 97.82%\n",
      "Test accuracy LoRA model: 97.82%\n",
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B: True\n",
      "Epoch: 001/005 | Batch 000/938 | Loss: 0.0062\n",
      "Epoch: 001/005 | Batch 400/938 | Loss: 0.0539\n",
      "Epoch: 001/005 | Batch 800/938 | Loss: 0.0365\n",
      "Epoch: 001/005 | Training Accuracy: 99.23%\n",
      "Time elapsed: 0.16 min\n",
      "Epoch: 002/005 | Batch 000/938 | Loss: 0.0740\n",
      "Epoch: 002/005 | Batch 400/938 | Loss: 0.0343\n",
      "Epoch: 002/005 | Batch 800/938 | Loss: 0.0056\n",
      "Epoch: 002/005 | Training Accuracy: 99.35%\n",
      "Time elapsed: 0.31 min\n",
      "Epoch: 003/005 | Batch 000/938 | Loss: 0.0120\n",
      "Epoch: 003/005 | Batch 400/938 | Loss: 0.0480\n",
      "Epoch: 003/005 | Batch 800/938 | Loss: 0.0054\n",
      "Epoch: 003/005 | Training Accuracy: 99.30%\n",
      "Time elapsed: 0.45 min\n",
      "Epoch: 004/005 | Batch 000/938 | Loss: 0.0287\n",
      "Epoch: 004/005 | Batch 400/938 | Loss: 0.0051\n",
      "Epoch: 004/005 | Batch 800/938 | Loss: 0.0586\n",
      "Epoch: 004/005 | Training Accuracy: 99.47%\n",
      "Time elapsed: 0.60 min\n",
      "Epoch: 005/005 | Batch 000/938 | Loss: 0.0323\n",
      "Epoch: 005/005 | Batch 400/938 | Loss: 0.0102\n",
      "Epoch: 005/005 | Batch 800/938 | Loss: 0.0651\n",
      "Epoch: 005/005 | Training Accuracy: 99.28%\n",
      "Time elapsed: 0.74 min\n",
      "Total Training Time: 0.74 min\n",
      "Test accuracy LoRA finetune: 97.78%\n",
      "Test accuracy orig model: 97.82%\n",
      "Test accuracy LoRA model: 97.78%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# LoRA Layer\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.A @ self.B) * self.alpha\n",
    "\n",
    "# Wrapper Linear Layer with LoRA\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 123\n",
    "torch.manual_seed(random_seed)\n",
    "x = torch.randn(1, 10)\n",
    "layer = nn.Linear(10, 5)\n",
    "print(x)\n",
    "print(layer)\n",
    "print('Original output:', layer(x))\n",
    "\n",
    "# Applying LoRA to Linear Layer\n",
    "layer_lora_1 = LinearWithLoRA(layer, rank=4, alpha=8)\n",
    "print(layer_lora_1(x))\n",
    "\n",
    "# Equivalent: Merged version\n",
    "class LinearWithLoRAMerged(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora = self.lora.A @ self.lora.B\n",
    "        combined_weight = self.linear.weight + self.lora.alpha * lora.T\n",
    "        return F.linear(x, combined_weight, self.linear.bias)\n",
    "\n",
    "layer_lora_2 = LinearWithLoRAMerged(layer, rank=4, alpha=8)\n",
    "print(layer_lora_2(x))\n",
    "\n",
    "# Multilayer Perceptron with 3 linear layers\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden_1, num_hidden_2, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Architecture parameters\n",
    "num_features = 28 * 28\n",
    "num_hidden_1 = 256\n",
    "num_hidden_2 = 128\n",
    "num_classes = 10\n",
    "\n",
    "# Training settings\n",
    "DEVICE = torch.device('mps' if torch.mps.is_available() else 'cpu')\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "\n",
    "model = MultilayerPerceptron(\n",
    "    num_features=num_features,\n",
    "    num_hidden_1=num_hidden_1,\n",
    "    num_hidden_2=num_hidden_2,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "optimizer_pretrained = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(DEVICE)\n",
    "print(model)\n",
    "print(optimizer_pretrained)\n",
    "\n",
    "# Dataset loading\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break\n",
    "\n",
    "# Evaluation function\n",
    "def compute_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.view(features.size(0), -1).to(device)\n",
    "            targets = targets.to(device)\n",
    "            logits = model(features)\n",
    "            _, predicted_labels = torch.max(logits, 1)\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum().item()\n",
    "    return correct_pred / num_examples * 100\n",
    "\n",
    "# Training loop\n",
    "def train(num_epochs, model, optimizer, train_loader, device):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            features = features.view(features.size(0), -1).to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            logits = model(features)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if not batch_idx % 400:\n",
    "                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} | Batch {batch_idx:03d}/{len(train_loader)} | Loss: {loss:.4f}')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            acc = compute_accuracy(model, train_loader, device)\n",
    "            print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} | Training Accuracy: {acc:.2f}%')\n",
    "\n",
    "        print(f'Time elapsed: {(time.time() - start_time) / 60:.2f} min')\n",
    "    print(f'Total Training Time: {(time.time() - start_time) / 60:.2f} min')\n",
    "\n",
    "# Train baseline model\n",
    "train(num_epochs, model, optimizer_pretrained, train_loader, DEVICE)\n",
    "print(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
    "\n",
    "# Replace Linear layers with LoRA\n",
    "model_lora = copy.deepcopy(model)\n",
    "model_lora.layers[0] = LinearWithLoRAMerged(model_lora.layers[0], rank=4, alpha=8)\n",
    "model_lora.layers[2] = LinearWithLoRAMerged(model_lora.layers[2], rank=4, alpha=8)\n",
    "model_lora.layers[4] = LinearWithLoRAMerged(model_lora.layers[4], rank=4, alpha=8)\n",
    "\n",
    "model_lora.to(DEVICE)\n",
    "optimizer_lora = torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
    "print(model_lora)\n",
    "\n",
    "print(f'Test accuracy orig model: {compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
    "print(f'Test accuracy LoRA model: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
    "\n",
    "# Freeze original linear weights\n",
    "def freeze_linear_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            freeze_linear_layers(child)\n",
    "\n",
    "freeze_linear_layers(model_lora)\n",
    "\n",
    "for name, param in model_lora.named_parameters():\n",
    "    print(f'{name}: {param.requires_grad}')\n",
    "\n",
    "# Train LoRA fine-tuned model\n",
    "optimizer_lora = torch.optim.Adam(filter(lambda p: p.requires_grad, model_lora.parameters()), lr=learning_rate)\n",
    "train(num_epochs, model_lora, optimizer_lora, train_loader, DEVICE)\n",
    "\n",
    "print(f'Test accuracy LoRA finetune: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n",
    "print(f'Test accuracy orig model: {compute_accuracy(model, test_loader, DEVICE):.2f}%')\n",
    "print(f'Test accuracy LoRA model: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
