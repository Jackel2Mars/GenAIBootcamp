{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c78461",
   "metadata": {},
   "source": [
    "Install Libraries:\n",
    "pip install rouge_score==0.1.2\n",
    "pip install evaluate\n",
    "pip install -U accelerate --quiet\n",
    "pip install datasets\n",
    "pip install nltk\n",
    "Download NLTK Resources:\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4346d003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75111a",
   "metadata": {},
   "source": [
    "Part II : Dataset Loading And Exploration\n",
    "\n",
    "Dataset Loading: Load the train.csv and test.csv datasets using pandas.\n",
    "Sampling: Take a smaller sample of the datasets (e.g., 100 samples from train, 50 from test) to reduce computational load.\n",
    "Exploration: Display the first example from the training sample, showing the article (prompt_text) and its reference summary (prompt_title).\n",
    "Data Inspection: Print the sampled train and test DataFrames to understand the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6306af7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∞ Article:\n",
      " Nasa has warned of an impending asteroid pass - and says it will be the closest until 2027. The asteroid, designated 2004 BL86, will safely pass about three times the distance of Earth to the moon on January 26. It will be the closest by any known space rock this large until asteroid 1999 AN10 flies past Earth in 2027. See the Asteroid's route below . At the time of its closest approach on January 26, the asteroid will be approximately 745,000 miles (1.2 million kilometers) from Earth. Due to its orbit around the sun, the asteroid is currently only visible by astronomers with large telescopes who are located in the southern hemisphere. But by Jan. 26, the space rock's changing position will make it visible to those in the northern hemisphere. From its reflected brightness, astronomers estimate that the asteroid is about a third of a mile (0.5 kilometers) in size. At the time of its closest approach on January 26, the asteroid will be approximately 745,000 miles (1.2 million kilometers) from Earth. 'Monday, January 26 will be the closest asteroid 2004 BL86 will get to Earth for at least the next 200 years,' said Don Yeomans, who is retiring as manager of NASA's Near Earth Object Program Office at the Jet Propulsion Laboratory in Pasadena, California, after 16 years in the position. 'And while it poses no threat to Earth for the foreseeable future, it's a relatively close approach by a relatively large asteroid, so it provides us a unique opportunity to observe and learn more.' One way NASA scientists plan to learn more about 2004 BL86 is to observe it with microwaves. NASA's Deep Space Network antenna at Goldstone, California, and the Arecibo Observatory in Puerto Rico will attempt to acquire science data and radar-generated images of the asteroid during the days surrounding its closest approach to Earth. 'When we get our radar data back the day after the flyby, we will have the first detailed images,' said radar astronomer Lance Benner of JPL, the principal investigator for the Goldstone radar observations of the asteroid. 'At present, we know almost nothing about the asteroid, so there are bound to be surprises.' Don't Panic! Nasa says 'While it poses no threat to Earth for the foreseeable future, it's a relatively close approach by a relatively large asteroid, so it provides us a unique opportunity to observe and learn more.' Asteroid 2004 BL86 was initially discovered on Jan. 30, 2004 by a telescope of the Lincoln Near-Earth Asteroid Research (LINEAR) survey in White Sands, New Mexico. The asteroid is expected to be observable to amateur astronomers with small telescopes and strong binoculars. 'I may grab my favorite binoculars and give it a shot myself,' said Yeomans. 'Asteroids are something special. Not only did asteroids provide Earth with the building blocks of life and much of its water, but in the future, they will become valuable resources for mineral ores and other vital natural resources. 'They will also become the fueling stops for humanity as we continue to explore our solar system. There is something about asteroids that makes me want to look up.'\n",
      "\n",
      "üìù Reference Summary:\n",
      " 2004 BL86 will pass about three times the distance of Earth to the moon . Estimate that the asteroid is about a third of a mile (0.5 kilometers) in size . Nasa says it poses no threat to Earth 'for the foreseeable future'\n",
      "\n",
      "üìä Train Sample Head:\n",
      "                                                  article  \\\n",
      "272581  Nasa has warned of an impending asteroid pass ...   \n",
      "772     BAGHDAD, Iraq (CNN) -- Iraq's most powerful Su...   \n",
      "171868  By . David Kent . Andy Carroll has taken an un...   \n",
      "63167   Los Angeles (CNN) -- Los Angeles has long been...   \n",
      "68522   London (CNN) -- Few shows can claim such an au...   \n",
      "\n",
      "                                               highlights  \\\n",
      "272581  2004 BL86 will pass about three times the dist...   \n",
      "772     Iraqi Islamic Party calls Quran incident \"blat...   \n",
      "171868  Carroll takes to Instagram to post selfie ahea...   \n",
      "63167   Pop stars from all over Europe are setting the...   \n",
      "68522   NEW: Young athletes light the Olympic cauldron...   \n",
      "\n",
      "                                              id  \n",
      "272581  6ccb7278e86893ad3609d30ecb5c9ea902fb9527  \n",
      "772     d4f57e3c18c38696345fb7a3d76a151bb9c5123b  \n",
      "171868  c9ae9fc314adcc92d3835b0437a1c44e9e233e1c  \n",
      "63167   5b5a383dc8f9487857787ced5426154394dd99db  \n",
      "68522   2813505a990ad24071496c0d0936e40847eb6194  \n",
      "\n",
      "üìä Test Sample Head:\n",
      "                                                 article  \\\n",
      "1516   Down Augusta way they say the azaleas are in f...   \n",
      "1393   There was no special treatment for Lewis Fergu...   \n",
      "10560  When emergency crews received a call saying 's...   \n",
      "11457  A loving boyfriend has granted his girlfriend ...   \n",
      "647    (CNN)Sunday's announcement that Corinthian Col...   \n",
      "\n",
      "                                              highlights  \\\n",
      "1516   Justin Rose bounced back from Florida misery b...   \n",
      "1393   Lewis Ferguson fell from Merrion Square at Win...   \n",
      "10560  Woman reported 'someone' had been run over, bu...   \n",
      "11457  Guo Kai and girlfriend Dong Hui, 22, had plann...   \n",
      "647    David Wheeler: Corinthian, considered a \"preda...   \n",
      "\n",
      "                                             id  \n",
      "1516   58aefdc7ca85968aa11e16ea4099506cb474f759  \n",
      "1393   8c2e48d24a3e2cf1be5d242f09ae34bf68ccbd6e  \n",
      "10560  16269bfc102681f55a7fbfb6e26c7a52d982e09c  \n",
      "11457  18514a002a1a244a68a560c63c4471af98f72a73  \n",
      "647    9efbe27504b041e7f5e846a3c6898702c0e82427  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"abisee/cnn_dailymail\", \"1.0.0\")\n",
    "\n",
    "# Step 2: Convert train and test splits to pandas DataFrames\n",
    "train_df = dataset[\"train\"].to_pandas()\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "\n",
    "# Step 3: Sample smaller subsets to reduce computational load\n",
    "train_sample = train_df.sample(n=100, random_state=42)\n",
    "test_sample = test_df.sample(n=50, random_state=42)\n",
    "\n",
    "# Step 4: Display the first example from the training sample\n",
    "first_example = train_sample.iloc[0]\n",
    "print(\"üì∞ Article:\\n\", first_example[\"article\"])\n",
    "print(\"\\nüìù Reference Summary:\\n\", first_example[\"highlights\"])\n",
    "\n",
    "# Step 5: Inspect the sampled DataFrames\n",
    "print(\"\\nüìä Train Sample Head:\")\n",
    "print(train_sample.head())\n",
    "\n",
    "print(\"\\nüìä Test Sample Head:\")\n",
    "print(test_sample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6596935",
   "metadata": {},
   "source": [
    "Part III : Summarization With T5\n",
    "\n",
    "Function Implementation: Implement the summarize_with_t5 function:\n",
    "Use T5ForConditionalGeneration and AutoTokenizer from transformers.\n",
    "Handle CUDA/MPS availability for GPU acceleration.\n",
    "Implement batch processing using the batch_generator function.\n",
    "Tokenize input articles with a ‚Äúsummarize: ‚Äù prefix.\n",
    "Generate summaries using model.generate().\n",
    "Decode generated token IDs back to text.\n",
    "Clear CUDA cache (torch.cuda.empty_cache()) and garbage collect (gc.collect()) after each batch and at the end of the function.\n",
    "Summary Generation: Generate summaries for the training sample using t5-small.\n",
    "Result Display: Display the generated summaries alongside the reference summaries in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1b3616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Article  \\\n",
      "0  Nasa has warned of an impending asteroid pass ...   \n",
      "1  BAGHDAD, Iraq (CNN) -- Iraq's most powerful Su...   \n",
      "2  By . David Kent . Andy Carroll has taken an un...   \n",
      "3  Los Angeles (CNN) -- Los Angeles has long been...   \n",
      "4  London (CNN) -- Few shows can claim such an au...   \n",
      "\n",
      "                                   Reference Summary  \\\n",
      "0  2004 BL86 will pass about three times the dist...   \n",
      "1  Iraqi Islamic Party calls Quran incident \"blat...   \n",
      "2  Carroll takes to Instagram to post selfie ahea...   \n",
      "3  Pop stars from all over Europe are setting the...   \n",
      "4  NEW: Young athletes light the Olympic cauldron...   \n",
      "\n",
      "                                   Generated Summary  \n",
      "0  the asteroid, designated 2004 BL86, will pass ...  \n",
      "1  a sniper section leader used a Quran for targe...  \n",
      "2  the striker is out for four months after teari...  \n",
      "3  aspiring pop stars from the u.s. and abroad ar...  \n",
      "4  a billion people around the world would be glu...  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "def batch_generator(data, batch_size):\n",
    "    \"\"\"Yield batches of data\"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i : i + batch_size]\n",
    "\n",
    "def summarize_with_t5(texts, batch_size=8):\n",
    "    \"\"\"\n",
    "    Summarizes a list of articles using T5-small model in batches.\n",
    "    \n",
    "    Args:\n",
    "        texts (list of str): Input articles to summarize.\n",
    "        batch_size (int): Number of samples to process per batch.\n",
    "    \n",
    "    Returns:\n",
    "        List of generated summaries.\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    model_name = \"t5-small\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    \n",
    "    # Use GPU if available (CUDA or Apple MPS)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():  # For Mac MPS\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    summaries = []\n",
    "    \n",
    "    for batch in batch_generator(texts, batch_size):\n",
    "        # Add \"summarize: \" prefix as T5 expects\n",
    "        inputs = [\"summarize: \" + article for article in batch]\n",
    "        \n",
    "        # Tokenize inputs (limit max length for speed & memory)\n",
    "        encoded = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = encoded.input_ids.to(device)\n",
    "        attention_mask = encoded.attention_mask.to(device)\n",
    "        \n",
    "        # Generate summaries\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids=input_ids,\n",
    "                                     attention_mask=attention_mask,\n",
    "                                     max_length=150,      # max summary length\n",
    "                                     num_beams=4,         # beam search for better summaries\n",
    "                                     early_stopping=True)\n",
    "        \n",
    "        # Decode to text\n",
    "        decoded_summaries = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in outputs]\n",
    "        summaries.extend(decoded_summaries)\n",
    "        \n",
    "        # Clear cache & garbage collect\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Final cleanup\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "#Summary Generation\n",
    "texts_to_summarize = train_sample[\"article\"].tolist()\n",
    "reference_summaries = train_sample[\"highlights\"].tolist()\n",
    "\n",
    "generated_summaries = summarize_with_t5(texts_to_summarize, batch_size=8)\n",
    "\n",
    "#Display results side-by-side\n",
    "df_results = pd.DataFrame({\n",
    "    \"Article\": texts_to_summarize,\n",
    "    \"Reference Summary\": reference_summaries,\n",
    "    \"Generated Summary\": generated_summaries\n",
    "})\n",
    "\n",
    "print(df_results.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbbf5f1",
   "metadata": {},
   "source": [
    "Part IV : Accuracy Evaluation\n",
    "\n",
    "Accuracy Calculation: Calculate the accuracy of the t5-small summaries by comparing them to the reference summaries.\n",
    "Result Interpretation: Print the calculated accuracy. Discuss why the accuracy is likely to be very low or zero, reinforcing the limitations of this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "721996dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact-match Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Calculate exact-match accuracy\n",
    "correct = sum(gen.strip() == ref.strip() for gen, ref in zip(generated_summaries, reference_summaries))\n",
    "total = len(reference_summaries)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Exact-match Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34b04c3",
   "metadata": {},
   "source": [
    "Exact-match accuracy is expected as close to 0 since: summarization is based semantics, and not necessarily on actual words beind the same.\n",
    "Therefore finding exact strings between a text and its summary is very unlikeley. Better metrics include ROUGE or human evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ab6f2",
   "metadata": {},
   "source": [
    "Part V : ROUGE Metric Implementation\n",
    "\n",
    "Metric Introduction: Introduce ROUGE (Recall-Oriented Understudy for Gisting Evaluation) as a standard metric for summarization.\n",
    "Library Usage: Load the rouge evaluation metric using evaluate.load(\"rouge\").\n",
    "Preprocessing: Explain the need to format the input summaries with newlines between sentences, and the use of the nltk sentence tokenizer.\n",
    "Function Definition: Create the compute_rouge_score function to calculate ROUGE scores, handling the necessary preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326c9f3d",
   "metadata": {},
   "source": [
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a standard automatic metric for evaluating summarization systems. It measures overlap between the generated summaries and reference summaries, focusing on:\n",
    "\n",
    "ROUGE-1: Overlap of unigrams (single words)\n",
    "ROUGE-2: Overlap of bigrams (pairs of words)\n",
    "ROUGE-L: Longest common subsequence\n",
    "It‚Äôs widely used because it correlates better with human judgment than exact-match accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa67734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def preprocess_summary(summary):\n",
    "    sentences = nltk.sent_tokenize(summary)\n",
    "    return \"\\n\".join(sentences)\n",
    "\n",
    "def compute_rouge_score(predictions, references):\n",
    "    \"\"\"\n",
    "    Compute ROUGE scores between generated summaries and reference summaries.\n",
    "\n",
    "    Args:\n",
    "        predictions (list of str): Generated summaries.\n",
    "        references (list of str): Reference summaries.\n",
    "\n",
    "    Returns:\n",
    "        dict: ROUGE scores.\n",
    "    \"\"\"\n",
    "    # Preprocess summaries to have newlines between sentences\n",
    "    preds = [preprocess_summary(p) for p in predictions]\n",
    "    refs = [preprocess_summary(r) for r in references]\n",
    "    \n",
    "    # Compute ROUGE\n",
    "    results = rouge.compute(predictions=preds, references=refs)\n",
    "    \n",
    "    # Return nicely rounded scores\n",
    "    return {k: round(v * 100, 2) for k, v in results.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4519d3",
   "metadata": {},
   "source": [
    "Part VI : Understanding ROUGE Scores\n",
    "\n",
    "Exact Match Test: Calculate ROUGE scores when the generated summaries are identical to the reference summaries.\n",
    "Null Prediction Test: Calculate ROUGE scores when the generated summaries are empty.\n",
    "Stemming Effect: Demonstrate the impact of stemming on ROUGE scores using simple examples.\n",
    "N-gram Analysis: Explore how ROUGE-1 and ROUGE-2 scores change with varying degrees of overlap between generated and reference summaries.\n",
    "Symmetry: Show the symmetry of rouge score with respect to predictions and references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e1ab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match ROUGE Scores:\n",
      "rouge1: 100.0\n",
      "rouge2: 100.0\n",
      "rougeL: 100.0\n",
      "rougeLsum: 100.0\n"
     ]
    }
   ],
   "source": [
    "#1. Exact Match Test\n",
    "\n",
    "exact_refs = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Dogs are great pets.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]\n",
    "exact_preds = exact_refs.copy()  # identical predictions\n",
    "\n",
    "exact_match_scores = compute_rouge_score(exact_preds, exact_refs)\n",
    "print(\"Exact Match ROUGE Scores:\")\n",
    "for metric, score in exact_match_scores.items():\n",
    "    print(f\"{metric}: {score}\")\n",
    "\n",
    "#Expected: ROUGE scores should be very high (close to 100), since predictions perfectly match references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f33463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null Prediction ROUGE Scores:\n",
      "rouge1: 0.0\n",
      "rouge2: 0.0\n",
      "rougeL: 0.0\n",
      "rougeLsum: 0.0\n"
     ]
    }
   ],
   "source": [
    "#2. Null Prediction Test\n",
    "null_preds = [\"\", \"\", \"\"]  # empty predictions\n",
    "\n",
    "null_scores = compute_rouge_score(null_preds, exact_refs)\n",
    "print(\"\\nNull Prediction ROUGE Scores:\")\n",
    "for metric, score in null_scores.items():\n",
    "    print(f\"{metric}: {score}\")\n",
    "\n",
    "#Expected: ROUGE scores will be very low or zero, reflecting no overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0765dd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming Effect ROUGE Scores:\n",
      "Exact match:\n",
      "rouge1: 100.0\n",
      "rouge2: 100.0\n",
      "rougeL: 100.0\n",
      "rougeLsum: 100.0\n",
      "\n",
      "Different verb form ('running' vs 'runs'):\n",
      "rouge1: 66.67\n",
      "rouge2: 28.57\n",
      "rougeL: 66.67\n",
      "rougeLsum: 66.67\n"
     ]
    }
   ],
   "source": [
    "#3. Stemming Effect\n",
    "refs_stem = [\"The cat is running fast.\"]\n",
    "preds_stem_1 = [\"The cat is running fast.\"]  # exact match\n",
    "preds_stem_2 = [\"The cat runs fast.\"]        # different form of 'run'\n",
    "\n",
    "scores_stem_1 = compute_rouge_score(preds_stem_1, refs_stem)\n",
    "scores_stem_2 = compute_rouge_score(preds_stem_2, refs_stem)\n",
    "\n",
    "print(\"\\nStemming Effect ROUGE Scores:\")\n",
    "print(\"Exact match:\")\n",
    "for metric, score in scores_stem_1.items():\n",
    "    print(f\"{metric}: {score}\")\n",
    "\n",
    "print(\"\\nDifferent verb form ('running' vs 'runs'):\")\n",
    "for metric, score in scores_stem_2.items():\n",
    "    print(f\"{metric}: {score}\")\n",
    "\n",
    "#Expected: The scores in the second case should still be reasonably high due to stemming matching ‚Äòrunning‚Äô and ‚Äòruns‚Äô."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "020e7da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-gram Overlap ROUGE Scores:\n",
      "\n",
      "Full overlap:\n",
      "{'rouge1': np.float64(100.0), 'rouge2': np.float64(100.0), 'rougeL': np.float64(100.0), 'rougeLsum': np.float64(100.0)}\n",
      "\n",
      "Partial overlap 1 (missing one word):\n",
      "{'rouge1': np.float64(90.91), 'rouge2': np.float64(88.89), 'rougeL': np.float64(90.91), 'rougeLsum': np.float64(90.91)}\n",
      "\n",
      "Partial overlap 2 (missing multiple words):\n",
      "{'rouge1': np.float64(80.0), 'rouge2': np.float64(25.0), 'rougeL': np.float64(80.0), 'rougeLsum': np.float64(80.0)}\n"
     ]
    }
   ],
   "source": [
    "#4. N-gram Analysis\n",
    "refs_ngrams = [\"The cat sat on the mat.\"]\n",
    "preds_full = [\"The cat sat on the mat.\"]  # full overlap\n",
    "preds_partial_1 = [\"The cat sat on the.\"] # missing last word\n",
    "preds_partial_2 = [\"The cat on mat.\"]     # missing multiple words\n",
    "\n",
    "print(\"\\nN-gram Overlap ROUGE Scores:\")\n",
    "\n",
    "print(\"\\nFull overlap:\")\n",
    "print(compute_rouge_score([preds_full[0]], [refs_ngrams[0]]))\n",
    "\n",
    "print(\"\\nPartial overlap 1 (missing one word):\")\n",
    "print(compute_rouge_score([preds_partial_1[0]], [refs_ngrams[0]]))\n",
    "\n",
    "print(\"\\nPartial overlap 2 (missing multiple words):\")\n",
    "print(compute_rouge_score([preds_partial_2[0]], [refs_ngrams[0]]))\n",
    "\n",
    "#Expected: ROUGE-1 scores decrease with less overlap, ROUGE-2 scores decrease even more because bigram matches are fewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d00f448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Symmetry Check:\n",
      "ROUGE(predictions, references): {'rouge1': np.float64(100.0), 'rouge2': np.float64(100.0), 'rougeL': np.float64(100.0), 'rougeLsum': np.float64(100.0)}\n",
      "ROUGE(references, predictions): {'rouge1': np.float64(100.0), 'rouge2': np.float64(100.0), 'rougeL': np.float64(100.0), 'rougeLsum': np.float64(100.0)}\n"
     ]
    }
   ],
   "source": [
    "#5. Symmetry Check\n",
    "refs_sym = [\"The cat sat on the mat.\"]\n",
    "preds_sym = [\"The cat sat on the mat.\"]\n",
    "\n",
    "scores_ref_pred = compute_rouge_score(preds_sym, refs_sym)\n",
    "scores_pred_ref = compute_rouge_score(refs_sym, preds_sym)\n",
    "\n",
    "print(\"\\nSymmetry Check:\")\n",
    "print(\"ROUGE(predictions, references):\", scores_ref_pred)\n",
    "print(\"ROUGE(references, predictions):\", scores_pred_ref)\n",
    "\n",
    "#Expected: Scores should be very close or equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28369594",
   "metadata": {},
   "source": [
    "Part VII : Comparing Small And Large Models\n",
    "\n",
    "Model Selection: Choose t5-small, t5-base, and gpt2 models.\n",
    "Summary Generation: Generate summaries for the training sample using each model.\n",
    "ROUGE Calculation: Calculate ROUGE scores for each model‚Äôs summaries using compute_rouge_score.\n",
    "Per-Row ROUGE: Create the compute_rouge_per_row function to calculate and store ROUGE scores for each individual article in a DataFrame.\n",
    "Result Display: Display the per-row ROUGE scores for each model.\n",
    "GPT2 Specifics: implement the summarize_with_gpt2 function, handling the ‚ÄúTL;DR:‚Äù prompt, and the token length limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de39b3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    T5ForConditionalGeneration, \n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer\n",
    ")\n",
    "import nltk\n",
    "import evaluate\n",
    "import gc\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4bcb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using CUDA GPU\")\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        try:\n",
    "            # Optional test to confirm MPS usability\n",
    "            _ = torch.tensor([1.0], device=\"mps\")\n",
    "            print(\"Using MPS GPU (macOS)\")\n",
    "            return torch.device(\"mps\")\n",
    "        except Exception as e:\n",
    "            print(f\"MPS available but not usable: {e}\")\n",
    "    print(\"Using CPU\")\n",
    "    return torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fb17560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "\n",
    "def summarize_with_t5_model(model_name, texts, batch_size=8, max_input_length=512, max_output_length=150):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    summaries = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=f\"Summarizing with {model_name}\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = [\"summarize: \" + text for text in batch]\n",
    "        encoding = tokenizer(inputs, padding=True, truncation=True, max_length=max_input_length, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=encoding[\"input_ids\"],\n",
    "                attention_mask=encoding[\"attention_mask\"],\n",
    "                max_length=max_output_length,\n",
    "                num_beams=2,  # Lowered for speed\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        decoded = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output]\n",
    "        summaries.extend(decoded)\n",
    "\n",
    "    return summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d4a8d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def summarize_with_gpt2(texts, batch_size=4, max_input_length=512, max_output_length=50):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    summaries = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Summarizing with GPT-2\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        prompts = [\"TL;DR: \" + text.strip() for text in batch]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, max_length=max_input_length, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=max_output_length,\n",
    "                num_beams=2,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        summaries.extend([text.split(\"TL;DR:\")[-1].strip() for text in decoded])\n",
    "\n",
    "    return summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1885e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def preprocess_summary(summary):\n",
    "    return \"\\n\".join(sent_tokenize(summary.strip()))\n",
    "\n",
    "def compute_rouge_per_row(predictions, references):\n",
    "    rows = []\n",
    "    for pred, ref in tqdm(zip(predictions, references), total=len(predictions), desc=\"Computing ROUGE\"):\n",
    "        score = rouge.compute(\n",
    "            predictions=[preprocess_summary(pred)],\n",
    "            references=[preprocess_summary(ref)],\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        rows.append({\n",
    "            \"ROUGE-1\": round(score[\"rouge1\"] * 100, 2),\n",
    "            \"ROUGE-2\": round(score[\"rouge2\"] * 100, 2),\n",
    "            \"ROUGE-L\": round(score[\"rougeL\"] * 100, 2),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c74c1a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS GPU (macOS)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing with t5-small: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:57<00:00,  4.42s/it]\n",
      "Computing ROUGE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 22.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS GPU (macOS)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing with t5-base: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [01:22<00:00,  6.35s/it]\n",
      "Computing ROUGE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 23.76it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac55a5802d814024a4e30b3856106230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7dd859cf6a462d8b33d4f08460feec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9002d1c9b3c94bdd8445044be358619a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3849b7fcce42ab9527ddffa0a744f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f056e6ec42c8434ea1143ba3bf6c801e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6a5fdd64c0419d892a678c3b1f23c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b28309b16146198fb98772e639c1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS GPU (macOS)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing with GPT-2:   4%|‚ñç         | 1/25 [00:06<02:31,  6.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:   8%|‚ñä         | 2/25 [00:12<02:19,  6.08s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  16%|‚ñà‚ñå        | 4/25 [00:16<01:12,  3.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  24%|‚ñà‚ñà‚ñç       | 6/25 [00:21<00:55,  2.90s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  40%|‚ñà‚ñà‚ñà‚ñà      | 10/25 [00:30<00:35,  2.37s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 11/25 [00:32<00:32,  2.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 12/25 [00:34<00:29,  2.28s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 13/25 [00:37<00:26,  2.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 14/25 [00:39<00:24,  2.24s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 15/25 [00:41<00:22,  2.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 17/25 [00:46<00:18,  2.28s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 18/25 [00:48<00:15,  2.26s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 20/25 [00:52<00:11,  2.25s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 22/25 [00:57<00:06,  2.21s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 23/25 [00:59<00:04,  2.20s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 24/25 [01:01<00:02,  2.18s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Summarizing with GPT-2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [01:03<00:00,  2.55s/it]\n",
      "Computing ROUGE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 20.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROUGE-1 (t5-small)  ROUGE-2 (t5-small)  ROUGE-L (t5-small)  \\\n",
      "0               47.50               33.33               42.50   \n",
      "1               26.97               13.79               20.22   \n",
      "2               25.64                5.26               12.82   \n",
      "3               34.92               22.58               26.98   \n",
      "4               36.36                6.98               29.55   \n",
      "\n",
      "   ROUGE-1 (t5-base)  ROUGE-2 (t5-base)  ROUGE-L (t5-base)  ROUGE-1 (gpt2)  \\\n",
      "0              47.37              35.14              44.74           16.44   \n",
      "1              43.96              20.22              26.37           16.12   \n",
      "2              28.99              14.93              23.19           11.55   \n",
      "3              38.38              22.68              34.34           17.37   \n",
      "4              33.77               8.00              23.38           11.43   \n",
      "\n",
      "   ROUGE-2 (gpt2)  ROUGE-L (gpt2)  \n",
      "0           14.93           16.44  \n",
      "1            8.30           11.57  \n",
      "2            6.00            9.16  \n",
      "3           13.39           13.33  \n",
      "4            2.87            8.16  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts = train_sample[\"article\"].tolist()\n",
    "references = train_sample[\"highlights\"].tolist()\n",
    "\n",
    "def evaluate_models(texts, references):\n",
    "    summaries_small = summarize_with_t5_model(\"t5-small\", texts)\n",
    "    df_small = compute_rouge_per_row(summaries_small, references)\n",
    "    df_small.columns = [\"ROUGE-1 (t5-small)\", \"ROUGE-2 (t5-small)\", \"ROUGE-L (t5-small)\"]\n",
    "\n",
    "    summaries_base = summarize_with_t5_model(\"t5-base\", texts)\n",
    "    df_base = compute_rouge_per_row(summaries_base, references)\n",
    "    df_base.columns = [\"ROUGE-1 (t5-base)\", \"ROUGE-2 (t5-base)\", \"ROUGE-L (t5-base)\"]\n",
    "\n",
    "    summaries_gpt2 = summarize_with_gpt2(texts)\n",
    "    df_gpt2 = compute_rouge_per_row(summaries_gpt2, references)\n",
    "    df_gpt2.columns = [\"ROUGE-1 (gpt2)\", \"ROUGE-2 (gpt2)\", \"ROUGE-L (gpt2)\"]\n",
    "\n",
    "    df_results = pd.concat([df_small, df_base, df_gpt2], axis=1)\n",
    "\n",
    "    model_outputs = {\n",
    "        \"t5-small\": summaries_small,\n",
    "        \"t5-base\": summaries_base,\n",
    "        \"gpt2\": summaries_gpt2\n",
    "    }\n",
    "\n",
    "    return df_results, model_outputs\n",
    "\n",
    "df_results, model_outputs = evaluate_models(texts, references)\n",
    "print(df_results.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed82982",
   "metadata": {},
   "source": [
    "üåü Part VIII : Comparing All Models\n",
    "\n",
    "Aggregation Function: Create the compare_models function to aggregate ROUGE scores for all models into a single DataFrame, showing average scores.\n",
    "Summary Comparison Function: Create the compare_models_summaries function to display the generated summaries from all models side-by-side in a DataFrame.\n",
    "Result Display: Display the aggregated ROUGE scores and the side-by-side summary comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46373225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(reference_summaries, model_outputs: dict):\n",
    "    \"\"\"\n",
    "    Compute average ROUGE scores for each model and return a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        reference_summaries (list of str): Ground-truth summaries.\n",
    "        model_outputs (dict): Keys are model names, values are lists of generated summaries.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Model-wise average ROUGE scores.\n",
    "    \"\"\"\n",
    "    avg_scores = {}\n",
    "    for model_name, preds in model_outputs.items():\n",
    "        df_rouge = compute_rouge_per_row(preds, reference_summaries)\n",
    "        avg = df_rouge.mean().round(2)\n",
    "        avg_scores[model_name] = avg.values  # ROUGE-1, ROUGE-2, ROUGE-L\n",
    "\n",
    "    return pd.DataFrame(avg_scores, index=[\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a81b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_summaries(texts, reference_summaries, model_outputs: dict, n=5):\n",
    "    \"\"\"\n",
    "    Show sample article, reference summary, and summaries from each model side-by-side.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): Input articles.\n",
    "        reference_summaries (list of str): Ground-truth summaries.\n",
    "        model_outputs (dict): Dict of model_name: list of summaries.\n",
    "        n (int): Number of rows to display.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Comparison of summaries per article.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"Article\": texts[:n],\n",
    "        \"Reference\": reference_summaries[:n]\n",
    "    }\n",
    "    for model_name, summaries in model_outputs.items():\n",
    "        data[model_name] = summaries[:n]\n",
    "    \n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59b11deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing ROUGE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 24.02it/s]\n",
      "Computing ROUGE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:04<00:00, 23.85it/s]\n",
      "Computing ROUGE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:05<00:00, 19.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Average ROUGE Scores for All Models:\n",
      "          ROUGE-1  ROUGE-2  ROUGE-L\n",
      "t5-small    37.45    17.17    27.30\n",
      "t5-base     39.92    19.71    29.14\n",
      "gpt2        16.97     9.04    11.80\n",
      "\n",
      "üìù Sample Summary Comparisons:\n",
      "                                             Article  \\\n",
      "0  Nasa has warned of an impending asteroid pass ...   \n",
      "1  BAGHDAD, Iraq (CNN) -- Iraq's most powerful Su...   \n",
      "2  By . David Kent . Andy Carroll has taken an un...   \n",
      "3  Los Angeles (CNN) -- Los Angeles has long been...   \n",
      "4  London (CNN) -- Few shows can claim such an au...   \n",
      "\n",
      "                                           Reference  \\\n",
      "0  2004 BL86 will pass about three times the dist...   \n",
      "1  Iraqi Islamic Party calls Quran incident \"blat...   \n",
      "2  Carroll takes to Instagram to post selfie ahea...   \n",
      "3  Pop stars from all over Europe are setting the...   \n",
      "4  NEW: Young athletes light the Olympic cauldron...   \n",
      "\n",
      "                                            t5-small  \\\n",
      "0  2004 BL86 will pass about three times the dist...   \n",
      "1  a sniper section leader used a Quran for targe...   \n",
      "2  the striker is out for four months after teari...   \n",
      "3  aspiring pop stars from the u.s. and abroad ar...   \n",
      "4  a billion people around the world would be glu...   \n",
      "\n",
      "                                             t5-base  \\\n",
      "0  2004 BL86 will pass about three times the dist...   \n",
      "1  u.s. soldier used the holy book for target pra...   \n",
      "2  england striker posts glum-looking selfie in h...   \n",
      "3  aspiring pop stars from all over Europe and fa...   \n",
      "4  \"Isles of Wonder\" opened the 2012 olympics in ...   \n",
      "\n",
      "                                                gpt2  \n",
      "0  Nasa has warned of an impending asteroid pass ...  \n",
      "1  BAGHDAD, Iraq (CNN) -- Iraq's most powerful Su...  \n",
      "2  By . David Kent . Andy Carroll has taken an un...  \n",
      "3  Los Angeles (CNN) -- Los Angeles has long been...  \n",
      "4  London (CNN) -- Few shows can claim such an au...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "texts = train_sample[\"article\"].tolist()\n",
    "references = train_sample[\"highlights\"].tolist()\n",
    "\n",
    "# ROUGE score comparison\n",
    "df_avg_rouge = compare_models(references, model_outputs)\n",
    "print(\"üî¢ Average ROUGE Scores for All Models:\")\n",
    "print(df_avg_rouge)\n",
    "\n",
    "# Side-by-side summary comparison\n",
    "df_sample_summaries = compare_models_summaries(texts, references, model_outputs, n=5)\n",
    "print(\"\\nüìù Sample Summary Comparisons:\")\n",
    "print(df_sample_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16203bbf",
   "metadata": {},
   "source": [
    "T5 models (especially fine-tuned versions) are explicitly trained for summarization tasks, so they naturally produce summaries that align better with reference highlights, yielding higher ROUGE scores.\n",
    "GPT-2, on the other hand, is a general-purpose language model trained mainly for language modeling (predicting the next token), not specifically for summarization. When you prompt it to summarize, it can generate plausible text, but it generally won't match references as closely as a model trained for summarization.\n",
    "Observing ROUGE scores ‚Äî T5 models scoring roughly twice as high as GPT-2 ‚Äî are perfectly expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
