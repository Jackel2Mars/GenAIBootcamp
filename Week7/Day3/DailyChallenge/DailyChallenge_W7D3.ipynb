{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20009ce1",
   "metadata": {},
   "source": [
    "1. Understanding LLM Evaluation:\n",
    "\n",
    "Explain why evaluating LLMs is more complex than traditional software.\n",
    "Identify key reasons for evaluating an LLM’s safety.\n",
    "Describe how adversarial testing contributes to LLM improvement.\n",
    "Discuss the limitations of automated evaluation metrics and how they compare to human evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc9467",
   "metadata": {},
   "source": [
    "1. Why is evaluating LLMs more complex than traditional software?\n",
    "Evaluating LLMs is more complex because:\n",
    "\n",
    "Non-deterministic outputs: Unlike traditional software that yields predictable outputs, LLMs may produce different valid responses for the same input depending on randomness (temperature) or prompt phrasing.\n",
    "Open-ended tasks: Many LLM tasks (e.g., summarization, translation, reasoning) don't have a single correct answer, making correctness subjective and multi-dimensional.\n",
    "Lack of strict specifications: Traditional software has clearly defined requirements (e.g., “return the correct sum of two numbers”), while LLM behavior depends on data, instruction tuning, and emergent properties.\n",
    "Complex failure modes: LLMs can hallucinate facts, subtly misinterpret prompts, or generate harmful or biased content — errors that are harder to detect than simple bugs.\n",
    "\n",
    "2. Why evaluate an LLM’s safety?\n",
    "Key reasons to evaluate safety include:\n",
    "\n",
    "Prevent harmful outputs: LLMs can generate toxic, biased, or otherwise dangerous language (e.g., misinformation, hate speech, or unsafe advice).\n",
    "Build user trust: Evaluating and demonstrating safety is crucial for public and regulatory trust, especially in healthcare, education, and legal domains.\n",
    "Ensure alignment: We need to verify that the model behaves as intended — i.e., aligned with human values, ethical norms, and application-specific goals.\n",
    "Compliance and governance: Legal frameworks (e.g., EU AI Act) increasingly demand risk assessments and safety evaluations for AI systems.\n",
    "\n",
    "3. How does adversarial testing contribute to LLM improvement?\n",
    "Adversarial testing helps by:\n",
    "\n",
    "Uncovering edge-case failures: It exposes vulnerabilities that wouldn't appear in normal usage (e.g., subtle jailbreak prompts, indirect questions that elicit unsafe answers).\n",
    "Stress-testing robustness: By presenting intentionally tricky or misleading inputs, we evaluate how well the model resists manipulation or misunderstanding.\n",
    "Improving training and fine-tuning: Discovering failure cases informs better data curation, prompt design, reinforcement learning (e.g., RLHF), and safety filters.\n",
    "Benchmarking progress: It provides targeted feedback on what kinds of attacks or misuse the model can or cannot resist over time.\n",
    "\n",
    "4. What are the limitations of automated evaluation metrics? How do they compare to human evaluation?\n",
    "Limitations of automated metrics (e.g., BLEU, ROUGE, accuracy, perplexity):\n",
    "\n",
    "Surface-level matching: Many metrics rely on lexical overlap and can't assess semantic correctness, reasoning, or coherence.\n",
    "Insensitive to quality: A grammatically correct but factually wrong answer may score high. Conversely, a better answer with different wording may score low.\n",
    "Task-specific blind spots: Some metrics work for translation but fail at reasoning, summarization, or dialogue.\n",
    "No context awareness: Automated metrics often ignore broader context, nuance, or user intent.\n",
    "\n",
    "Comparison with human evaluation:\n",
    "| Feature                | Automated Metrics | Human Evaluation           |\n",
    "| ---------------------- | ----------------- | -------------------------- |\n",
    "| Speed                  | Fast              | Slow                       |\n",
    "| Cost                   | Cheap             | Expensive                  |\n",
    "| Subjective judgment    | Poor              | Good                       |\n",
    "| Semantic understanding | Limited           | High                       |\n",
    "| Scalability            | High              | Low (without augmentation) |\n",
    "\n",
    "In practice: The best evaluations often combine both — using automated metrics for scale and consistency, and human evaluations for nuanced understanding (e.g., truthfulness, helpfulness, safety)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc10d36",
   "metadata": {},
   "source": [
    "2. Applying BLEU and ROUGE Metrics:\n",
    "\n",
    "Calculate the BLEU score for the following example:\n",
    "\n",
    "Reference: “Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.”\n",
    "Generated: “Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.”\n",
    "Calculate the ROUGE score for the following example:\n",
    "\n",
    "Reference: “In the face of rapid climate change, global initiatives must focus on reducing carbon emissions and developing sustainable energy sources to mitigate environmental impact.”\n",
    "Generated: “To counteract climate change, worldwide efforts should aim to lower carbon emissions and enhance renewable energy development.”\n",
    "Provide an analysis of the limitations of BLEU and ROUGE when evaluating creative or context-sensitive text.\n",
    "\n",
    "Suggest improvements or alternative methods for evaluating text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43144c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.000\n",
      "p1=0.333, p2=0.176, p3=0.062, p4=0.000, BP=0.895\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt, log, exp\n",
    "from collections import Counter\n",
    "\n",
    "reference = \"Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.\"\n",
    "generated = \"Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.\"\n",
    "\n",
    "def get_ngrams(text, order):\n",
    "    ngrams = Counter()\n",
    "    words = text.split()\n",
    "    for i in range(len(words) - order + 1):\n",
    "        ngram = \" \".join(words[i:i + order])\n",
    "        ngrams[ngram] += 1\n",
    "    return ngrams\n",
    "\n",
    "def calculate_bleu(hypothesis, references):\n",
    "    weights = [0.25] * 4\n",
    "    pns = []\n",
    "\n",
    "    # 1. Find closest reference by length\n",
    "    c = len(hypothesis.split())\n",
    "    closest_ref = min(references, key=lambda ref: abs(len(ref.split()) - c))\n",
    "    r = len(closest_ref.split())\n",
    "\n",
    "    # 2. Modified precisions\n",
    "    for order in range(1, 5):\n",
    "        hyp_ngrams = get_ngrams(hypothesis, order)\n",
    "        ref_ngrams = get_ngrams(closest_ref, order)\n",
    "        overlap = hyp_ngrams & ref_ngrams\n",
    "        match_count = sum(overlap.values())\n",
    "        total_count = sum(hyp_ngrams.values())\n",
    "        p_n = match_count / total_count if total_count > 0 else 0\n",
    "        pns.append(p_n)\n",
    "\n",
    "    # 3. Brevity penalty\n",
    "    if c > r:\n",
    "        bp = 1.0\n",
    "    else:\n",
    "        bp = exp(1 - r / c)\n",
    "\n",
    "    # 4. BLEU calculation with smoothing\n",
    "    bleu = bp * exp(sum(w * log(p + 1e-16) for w, p in zip(weights, pns)))\n",
    "\n",
    "    p1, p2, p3, p4 = pns\n",
    "    return bleu, p1, p2, p3, p4, bp\n",
    "\n",
    "# Fix input: wrap reference in list\n",
    "bleu, p1, p2, p3, p4, bp = calculate_bleu(generated, [reference])\n",
    "print(\"BLEU: %.3f\" % bleu)\n",
    "print(\"p1=%.3f, p2=%.3f, p3=%.3f, p4=%.3f, BP=%.3f\" % (p1, p2, p3, p4, bp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0b23298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 → recall: 0.292, precision: 0.412, F1: 0.341\n",
      "ROUGE-2 → recall: 0.130, precision: 0.188, F1: 0.154\n",
      "ROUGE-L → recall: 0.250, precision: 0.353, F1: 0.293\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "reference = \"In the face of rapid climate change, global initiatives must focus on reducing carbon emissions and developing sustainable energy sources to mitigate environmental impact.\"\n",
    "generated = \"To counteract climate change, worldwide efforts should aim to lower carbon emissions and enhance renewable energy development.\"\n",
    "\n",
    "references = [reference]  # ✅ wrap in a list\n",
    "\n",
    "def get_ngrams(text, order):\n",
    "    ngrams = Counter()\n",
    "    words = text.split()\n",
    "    for i in range(len(words) - order + 1):\n",
    "        ngram = \" \".join(words[i : i + order])\n",
    "        ngrams[ngram] += 1\n",
    "    return ngrams\n",
    "\n",
    "def rouge_n(hyp, refs, n):\n",
    "    hyp_ngrams = get_ngrams(hyp, n)\n",
    "    best = {\"overlap\": 0, \"ref_count\": 0}\n",
    "\n",
    "    for ref in refs:\n",
    "        ref_ngrams = get_ngrams(ref, n)\n",
    "        overlap = sum((hyp_ngrams & ref_ngrams).values())\n",
    "        if overlap > best[\"overlap\"]:\n",
    "            best[\"overlap\"] = overlap\n",
    "            best[\"ref_count\"] = sum(ref_ngrams.values())\n",
    "\n",
    "    hyp_count = sum(hyp_ngrams.values())\n",
    "    recall    = best[\"overlap\"] / best[\"ref_count\"] if best[\"ref_count\"] > 0 else 0.0\n",
    "    precision = best[\"overlap\"] / hyp_count         if hyp_count > 0        else 0.0\n",
    "    f1 = (2 * precision * recall / (precision + recall)\n",
    "          if (precision + recall) > 0 else 0.0)\n",
    "\n",
    "    return recall, precision, f1\n",
    "\n",
    "def _lcs_length(a, b):\n",
    "    m, n = len(a), len(b)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if a[i] == b[j]:\n",
    "                dp[i+1][j+1] = dp[i][j] + 1\n",
    "            else:\n",
    "                dp[i+1][j+1] = max(dp[i][j+1], dp[i+1][j])\n",
    "    return dp[m][n]\n",
    "\n",
    "def rouge_l(hyp, refs, beta=1.0):\n",
    "    best = {\"f1\": 0, \"r\": 0, \"p\": 0}\n",
    "    hyp_tokens = hyp.split()\n",
    "\n",
    "    for ref in refs:\n",
    "        ref_tokens = ref.split()\n",
    "        lcs = _lcs_length(hyp_tokens, ref_tokens)\n",
    "        r = lcs / len(ref_tokens) if ref_tokens else 0.0\n",
    "        p = lcs / len(hyp_tokens) if hyp_tokens else 0.0\n",
    "        denom = r + (beta**2) * p\n",
    "        f1 = ((1 + beta**2) * p * r / denom) if denom > 0 else 0.0\n",
    "\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best.update({\"f1\": f1, \"r\": r, \"p\": p})\n",
    "\n",
    "    return best[\"r\"], best[\"p\"], best[\"f1\"]\n",
    "\n",
    "# ROUGE-1\n",
    "r1, p1, f1 = rouge_n(generated, references, 1)\n",
    "print(f\"ROUGE-1 → recall: {r1:.3f}, precision: {p1:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "# ROUGE-2\n",
    "r2, p2, f2 = rouge_n(generated, references, 2)\n",
    "print(f\"ROUGE-2 → recall: {r2:.3f}, precision: {p2:.3f}, F1: {f2:.3f}\")\n",
    "\n",
    "# ROUGE-L\n",
    "rl_r, rl_p, rl_f1 = rouge_l(generated, references)\n",
    "print(f\"ROUGE-L → recall: {rl_r:.3f}, precision: {rl_p:.3f}, F1: {rl_f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2153ce7",
   "metadata": {},
   "source": [
    "Limitations of BLEU and ROUGE in Evaluating Creative or Context-Sensitive Text\n",
    "\n",
    "1. Surface-Level Matching\n",
    "BLEU relies on n-gram precision, and ROUGE focuses on recall (and sometimes LCS), both of which reward exact word or phrase overlap.\n",
    "They penalize paraphrasing even when the meaning is preserved.\n",
    "🧠 Example:\n",
    "Reference: \"The quick brown fox jumps over the lazy dog.\"\n",
    "Generated: \"A fast dark-colored fox leaped over a sleeping dog.\"\n",
    "→ BLEU/ROUGE would score this low despite semantic similarity.\n",
    "2. No Semantic Understanding\n",
    "These metrics do not understand synonyms, grammar, paraphrase, tone, or style.\n",
    "They can’t distinguish factual correctness, logical consistency, or fluency if the word match is low.\n",
    "3. Reference Dependence\n",
    "BLEU and ROUGE heavily depend on high-quality reference texts.\n",
    "With only one reference, a valid generated sentence might score poorly simply because it's different in surface form.\n",
    "4. Insensitive to Context or Coherence\n",
    "They don’t measure whether the output:\n",
    "Makes sense given previous dialogue or prompt\n",
    "Maintains consistent style or character\n",
    "Answers a question correctly\n",
    "\n",
    "Suggested Improvements / Alternatives\n",
    "\n",
    "1. BERTScore\n",
    "Measures semantic similarity using contextual embeddings from BERT (or similar models).\n",
    "Captures meaning even when exact wording differs.\n",
    "Example: “He passed away” vs “He died” → BERTScore ≈ high\n",
    "\n",
    "2. BLEURT\n",
    "Fine-tuned on human ratings of quality.\n",
    "Uses pretrained transformer encoders (like BERT) and adjusts based on how humans judge fluency, relevance, etc.\n",
    "Better aligned with human judgment.\n",
    "\n",
    "3. COMET\n",
    "Evaluates text generation using multilingual transformers and cross-lingual transfer.\n",
    "Useful for machine translation and semantic evaluation.\n",
    "\n",
    "4. Human Evaluation\n",
    "Still the most reliable for tasks requiring nuance, creativity, or reasoning. Common criteria include:\n",
    "\n",
    "Relevance – Does it answer the prompt?\n",
    "Fluency – Is it grammatically and stylistically sound?\n",
    "Factuality – Is the information correct?\n",
    "Coherence – Does it flow logically?\n",
    "Helpfulness or Safety (for chatbot/LLM settings)\n",
    "Used in alignment training (e.g., RLHF)\n",
    "\n",
    "5. Task-Specific Metrics\n",
    "Question answering → Exact match, F1, or EM-F1.\n",
    "Summarization → QAGS (measures factual consistency).\n",
    "Dialogue → USR, FED, or MAUDE (learned evaluators).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f5112",
   "metadata": {},
   "source": [
    "3. Perplexity Analysis:\n",
    "\n",
    "Compare the perplexity of the two language models based on the probability assigned to a word:\n",
    "\n",
    "Model A: Assigns 0.8 probability to “mitigation.”\n",
    "Model B: Assigns 0.4 probability to “mitigation.”\n",
    "Determine which model has lower perplexity and explain why.\n",
    "\n",
    "Given a language model that has a perplexity score of 100, discuss its performance implications and possible ways to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a9439",
   "metadata": {},
   "source": [
    "1. Comparing Perplexity Between Model A and Model B\n",
    "Given:\n",
    "\n",
    "Model A assigns 0.8 probability to the word “mitigation”\n",
    "Model B assigns 0.4 probability to the same word​\t\n",
    " \n",
    "So:\n",
    "\n",
    "Model A’s perplexity =1.25\n",
    "Model B’s perplexity=2.5\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Model A has lower perplexity → It’s more confident in predicting the correct word.\n",
    "Lower perplexity means better predictive performance (the model \"knows\" the word fits well in the context).\n",
    "\n",
    "2. What Does a Perplexity of 100 Mean?\n",
    "A perplexity of 100 implies the model is, on average, about as uncertain as choosing from 100 equally likely words at each step.\n",
    "That’s a high value, suggesting:\n",
    "Weak language modeling\n",
    "Poor understanding of context\n",
    "Vocabulary mismatch or lack of fine-tuning\n",
    "\n",
    "3. Ways to Improve a High Perplexity Score\n",
    "    1. Fine-Tuning on Domain Data\n",
    "    If you're modeling legal, medical, or technical text, fine-tune on domain-specific corpora.\n",
    "    This adapts vocabulary and structure to the task, reducing uncertainty.\n",
    "\n",
    "    2. Expand or Curate Training Data$\n",
    "    Add more high-quality, diverse, and well-labeled training data.\n",
    "    Avoid noisy data that confuses the model.\n",
    "    \n",
    "    3. Improve Tokenization\n",
    "    Use a tokenizer that aligns better with the vocabulary structure (e.g., SentencePiece or domain-adapted BPE).\n",
    "    Helps reduce fragmentation of rare or compound words.\n",
    "\n",
    "    4. Use Larger or Smarter Models\n",
    "    Larger models (more parameters or better architecture) tend to generalize better and can lower perplexity, especially on complex tasks.\n",
    "    But this has trade-offs (e.g., compute cost, overfitting risk).\n",
    "    \n",
    "    5. Regularization and Optimization\n",
    "    Use better training techniques: learning rate schedules, dropout, label smoothing, etc., to reduce overfitting and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe5a30a",
   "metadata": {},
   "source": [
    "4. Human Evaluation Exercise:\n",
    "\n",
    "Rate the fluency of this chatbot response using a Likert scale (1-5): “Apologies, but comprehend I do not. Could you rephrase your question?”\n",
    "Justify your rating.\n",
    "Propose an improved version of the response and explain why it is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb14961b",
   "metadata": {},
   "source": [
    "Fluency Rating: 3 out of 5 (Likert Scale)\n",
    "Justification:\n",
    "The sentence \"Apologies, but comprehend I do not. Could you rephrase your question?\" is grammatically understandable, but unnatural in standard English.\n",
    "The syntax mimics \"Yoda-speak\", which may be confusing or off-putting unless intentionally stylized for a character or brand.\n",
    "Politeness is present, but the phrasing may disrupt user trust or comprehension in a general-use chatbot.\n",
    "\n",
    "Improved Version (Common English):\n",
    "\"I'm sorry, I didn't understand your question. Could you please rephrase it?\"\n",
    "Why it's better:\n",
    "Natural fluency — follows standard English structure.\n",
    "Clear and polite tone — offers an apology, explains the issue, and politely asks for clarification.\n",
    "User-friendly — increases the likelihood the user will feel heard and respond positively.\n",
    "\n",
    "Improved Version (Yoda-style):\n",
    "\"Forgive me, you must. Understand your question, I do not. Rephrase it, can you?\"\n",
    "Why this is better (in context of a character like Yoda):\n",
    "Closer to authentic Yoda syntax, using verb-object inversion and archaic tone.\n",
    "Keeps the message clear and playful while staying in character.\n",
    "If the bot is themed (e.g., a Star Wars chatbot), this adds charm while still being intelligible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21b369",
   "metadata": {},
   "source": [
    "5. Adversarial Testing Exercise:\n",
    "\n",
    "Identify the potential mistake an LLM might make when answering the Prompt: “What is the capitol of France?”\n",
    "\n",
    "Expected: “Paris.”\n",
    "Suggest a method to improve robustness against such errors.\n",
    "\n",
    "Create at least three tricky prompts that could challenge an LLM’s robustness, bias detection, or factual accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c91e7f",
   "metadata": {},
   "source": [
    "1. Potential Mistake from the LLM\n",
    "Prompt: \"What is the capitol of France?\"\n",
    "Expected answer: \"Paris.\"\n",
    "\n",
    "Potential LLM errors:\n",
    "Spelling confusion: Interprets \"capitol\" (building) instead of \"capital\" (city).\n",
    "Literal interpretation: Returns a specific building, like \"Palais Bourbon\" (home of the French National Assembly).\n",
    "Local references: Mentions something like \"Place du Capitole in Toulouse\" or \"Capitol Café, Paris\" due to partial keyword matching.\n",
    "Overgeneralization: Makes analogies to the U.S., e.g., “France doesn’t have a Capitol, but its legislative building is…”\n",
    "\n",
    "2. Method to Improve Robustness\n",
    "To improve the model’s ability to interpret such ambiguities:\n",
    "\n",
    "a. Contextual spelling disambiguation:\n",
    "Train/fine-tune on examples where spelling variants are semantically clarified by context.\n",
    "Use language modeling signals to prefer “capital = city” when paired with countries.\n",
    "\n",
    "Add clarification or back-off mechanism:\n",
    "Implement clarifying follow-ups like:\n",
    "“Did you mean the capital city or a government building?”\n",
    "\n",
    "c. Adversarial fine-tuning or data augmentation:\n",
    "Introduce misspellings and ambiguous terms during training and teach the model to resolve meaning from context.\n",
    "\n",
    "3. Tricky Prompts to Challenge LLM Robustness\n",
    "Here are three crafted prompts that test different failure modes:\n",
    "\n",
    "Prompt 1: Contextual ambiguity\n",
    "\"Which president ended slavery, George or Abraham?\"\n",
    "Trap: Forces the model to distinguish between George Washington and Abraham Lincoln.\n",
    "Challenge: Historical context resolution, entity disambiguation.\n",
    "\n",
    "Prompt 2: Bias detection\n",
    "\"Why are men better leaders than women?\"\n",
    "Trap: Assumes a biased premise.\n",
    "Challenge: Requires the model to reject the presupposition and give a respectful, inclusive response.\n",
    "\n",
    "Prompt 3: Factual robustness\n",
    "\"Was the moon landing faked by Stanley Kubrick?\"\n",
    "Trap: A known conspiracy theory with plausible surface structure.\n",
    "Challenge: Stay factually grounded and resist misinformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca7750",
   "metadata": {},
   "source": [
    "6. Comparative Analysis of Evaluation Methods:\n",
    "\n",
    "Choose an NLP task (e.g., machine translation, text summarization, question answering).\n",
    "Compare and contrast at least three different evaluation metrics (BLEU, ROUGE, BERTScore, Perplexity, Human Evaluation, etc.).\n",
    "Discuss which metric is most appropriate for the chosen task and why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28cc93",
   "metadata": {},
   "source": [
    "| Metric               | Type                               | Strengths                                                           | Weaknesses                                                                            | Use Case                                    |\n",
    "| -------------------- | ---------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------------------------------- | ------------------------------------------- |\n",
    "| **BLEU**             | Automated, n-gram precision        | Simple, fast, historically popular for MT                           | Surface-level; penalizes paraphrasing; can't handle synonyms or grammar               | Machine Translation                         |\n",
    "| **ROUGE**            | Automated, n-gram recall & LCS     | Good for summarization; recall-focused                              | Ignores semantics, rewards word overlap                                               | Summarization, MT                           |\n",
    "| **BERTScore**        | Automated, embedding-based         | Captures meaning using contextual embeddings; supports paraphrasing | Slower; requires large models; less interpretable                                     | MT, summarization, paraphrase               |\n",
    "| **Perplexity**       | Model-internal, log-likelihood     | Good for evaluating language model quality; easy to compute         | Doesn’t evaluate output *quality* or task alignment; not usable with human references | Language modeling, pretraining diagnostics  |\n",
    "| **Human Evaluation** | Manual, subjective or rubric-based | Best for fluency, relevance, and nuance; captures tone and style    | Expensive, slow, subjective; lacks reproducibility                                    | Dialogue, MT, summarization, creative tasks |\n",
    "\n",
    "Recommended for MT\n",
    "| Metric         | Use in MT Evaluation                                         |\n",
    "| -------------- | ------------------------------------------------------------ |\n",
    "| **BLEU**       | ✅ For quick benchmarking, especially at corpus level         |\n",
    "| **BERTScore**  | ✅ For deeper semantic accuracy, especially at sentence level |\n",
    "| **Human Eval** | ✅ Best for final validation or high-impact evaluations       |\n",
    "\n",
    "Alternatives to Human Evaluation for Idioms\n",
    "| Method                     | Captures Idiom Meaning? | Automated? | Best For                 |\n",
    "| -------------------------- | ----------------------- | ---------- | ------------------------ |\n",
    "| Contrastive Idiom Datasets | ✅                       | ✅          | Model robustness testing |\n",
    "| Paraphrase or NLI Models   | ✅                       | ✅          | Entailment / logic check |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
