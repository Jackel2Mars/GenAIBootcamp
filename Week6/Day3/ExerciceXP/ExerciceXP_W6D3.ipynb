{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1: Tokenization With BERT\n",
        "\n",
        "Objective: Learn how BERT tokenizes text and adds special tokens, preparing it for model input.\n",
        "\n",
        "Why this matters:\n",
        "Before any language model can process text, it needs to convert it into tokens and numerical IDs. BERT uses special tokens like [CLS] and [SEP] to mark the beginning and end of sentences. This exercise helps you understand how BERT prepares raw text for analysis.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Install the transformers and torch libraries.\n",
        "Load the BERT tokenizer (bert-base-uncased).\n",
        "Choose a sample sentence.\n",
        "Tokenize the sentence and view how BERT breaks it down.\n",
        "Prepare the sentence with special tokens, padding, and truncation for model input.\n",
        "Review the token IDs and tokens, identifying the special tokens BERT adds.\n",
        "Outcome: You will have a fully tokenized sentence, see the special tokens BERT adds, and understand how text becomes input for BERT."
      ],
      "metadata": {
        "id": "s_GqW7xvmtK4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbwjtIQDmsYs",
        "outputId": "05efd97e-607c-4854-c2ba-046fee49ea00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens for input 0: ['[CLS]', 'how', 'can', 'you', 'leave', 'me', 'standing', 'alone', 'in', 'a', 'world', 'that', \"'\", 's', 'so', 'cold', '?', 'maybe', 'i', \"'\", 'm', 'just', 'too', 'demanding', '.', '.', '.', 'maybe', 'i', \"'\", 'm', 'just', 'like', 'my', 'father', ',', 'too', 'bored', '!', '[SEP]']\n",
            "Tokens for input 0: ['[CLS]', 'how', 'can', 'you', 'leave', 'me', 'standing', 'alone', 'in', 'a', 'world', 'that', \"'\", 's', 'so', 'cold', '?', 'maybe', 'i', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "#Install the transformers and torch libraries\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "#Load the BERT tokenizer (bert-base-uncased)\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Choose a sample sentence.\n",
        "text = \"How can you leave me standing alone in a world that's so cold? Maybe I'm just too demanding... Maybe I'm just like my father, too bored!\"\n",
        "\n",
        "#Tokenize the sentence and view how BERT breaks it down\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "for i in range(encoded_input['input_ids'].shape[0]):\n",
        "    input_ids = encoded_input['input_ids'][i]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    print(f\"Tokens for input {i}:\", tokens)\n",
        "\n",
        "#Prepare the sentence with special tokens, padding, and truncation for model input.\n",
        "encoded_input = tokenizer(\n",
        "    text,\n",
        "    padding='max_length',       # pad to max_length\n",
        "    truncation=True,            # truncate if needed\n",
        "    max_length=20,              # example fixed max length\n",
        "    return_tensors='tf'         # return TensorFlow tensors\n",
        ")\n",
        "\n",
        "#Review the token IDs and tokens, identifying the special tokens BERT adds.\n",
        "for i in range(encoded_input['input_ids'].shape[0]):\n",
        "    input_ids = encoded_input['input_ids'][i]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    print(f\"Tokens for input {i}:\", tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üåü Exercise 2: Sentiment Analysis With BERT Pipeline\n",
        "\n",
        "Objective: Use a pre-trained BERT model to perform sentiment analysis.\n",
        "\n",
        "Why this matters:\n",
        "\n",
        "Pre-trained models like BERT can quickly classify text, such as determining if a sentence is positive or negative. Pipelines simplify this process, allowing you to focus on the task without managing low-level details.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Import the pipeline class from transformers.\n",
        "Create a sentiment analysis pipeline using the distilbert-base-uncased-finetuned-sst-2-english model.\n",
        "Provide a sample sentence.\n",
        "Use the pipeline to predict the sentiment.\n",
        "Review the predicted label and confidence score.\n",
        "Outcome: You will have a working sentiment analysis pipeline that can classify text as positive or negative."
      ],
      "metadata": {
        "id": "4SmM8Xq_sUdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the pipeline class from transformers.\n",
        "from transformers import pipeline\n",
        "\n",
        "#Create a sentiment analysis pipeline using the distilbert-base-uncased-finetuned-sst-2-english model.\n",
        "analyzer = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "#Provide a sample sentence.\n",
        "sample_sentence= \"When I walk through the valley of the shadow of death, I don't feel no evil!\"\n",
        "\n",
        "#Use the pipeline to predict the sentiment.\n",
        "result = analyzer(sample_sentence)\n",
        "print(result)\n",
        "\n",
        "#Review the predicted label and confidence score.\n",
        "label = result[0]['label']\n",
        "score = result[0]['score']\n",
        "\n",
        "print(f\"Predicted Sentiment: {label}\")\n",
        "print(f\"Confidence Score: {score:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfH--imysVzl",
        "outputId": "23cabd34-9fd9-4919-df49-86cd187fcdc9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.9990573525428772}]\n",
            "Predicted Sentiment: POSITIVE\n",
            "Confidence Score: 0.9991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üåü Exercise 3: Building A Custom Sentiment Analyzer\n",
        "\n",
        "Objective: Build a sentiment analyzer with direct control over the tokenizer, model, and processing pipeline.\n",
        "\n",
        "Why this matters:\n",
        "\n",
        "Using pipelines is convenient, but building a custom analyzer helps you understand how models process inputs and generate outputs. You gain full control over preprocessing, model handling, and post-processing.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. Import AutoTokenizer and AutoModelForSequenceClassification.\n",
        "2. Create a class BERTSentimentAnalyzer with methods for:\n",
        "\n",
        "Initializing the tokenizer and model.\n",
        "Preprocessing input text (cleaning, tokenizing, preparing tensors).\n",
        "Predicting sentiment and returning results.\n",
        "3. Test your analyzer with various sample texts.\n",
        "\n",
        "Outcome: You will have a custom sentiment analyzer and understand each component‚Äôs role in the pipeline."
      ],
      "metadata": {
        "id": "D01sW-90xJUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import AutoTokenizer and AutoModelForSequenceClassification.\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import re\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "#Create a class BERTSentimentAnalyzer with methods for:\n",
        "class BERTSentimentAnalyzer:\n",
        "  #Initializing the tokenizer and model\n",
        "  def __init__(self, model_name='distilbert-base-uncased-finetuned-sst-2-english'):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "  #Preprocessing input text (cleaning, tokenizing, preparing tensors)\n",
        "\n",
        "  def preprocess_input_text(self, text):\n",
        "      # Basic cleaning (optional)\n",
        "      text = text.strip()\n",
        "      text = re.sub(r'\\s+', ' ', text)  # collapse multiple spaces\n",
        "      text = re.sub(r'[^\\w\\s\\'!?.,]', '', text)  # remove unwanted characters\n",
        "\n",
        "      # Tokenize and prepare tensors\n",
        "      inputs = self.tokenizer(\n",
        "          text,\n",
        "          return_tensors='pt',\n",
        "          truncation=True,\n",
        "          padding=True\n",
        "      )\n",
        "      return inputs\n",
        "\n",
        "  #Predicting sentiment and returning results.\n",
        "  def predict_sentiment(self, text):\n",
        "      # Preprocess: tokenize and prepare tensors\n",
        "      inputs = self.preprocess_input_text(text)\n",
        "\n",
        "      # Run model (disable gradient calculations)\n",
        "      with torch.no_grad():\n",
        "          outputs = self.model(**inputs)\n",
        "\n",
        "      # Get logits and convert to probabilities\n",
        "      probs = F.softmax(outputs.logits, dim=1)\n",
        "\n",
        "      # Get predicted class index and confidence\n",
        "      predicted_class_idx = torch.argmax(probs, dim=1).item()\n",
        "      confidence = probs[0][predicted_class_idx].item()\n",
        "\n",
        "      # Convert class index to label (e.g., 'POSITIVE', 'NEGATIVE')\n",
        "      label = self.model.config.id2label[predicted_class_idx]\n",
        "\n",
        "      return {'label': label, 'confidence': round(confidence, 4)}\n",
        "\n",
        "#Test your analyzer with various sample texts.\n",
        "sample_texts= [\n",
        "    \"Who lives by the sword will die by the sword\",\n",
        "    \"Even a broken watch gives the right time twice a day\",\n",
        "    \"Women can keep a secret, as long as they cooperate on it\"\n",
        "]\n",
        "\n",
        "bert_sentime_analyzer = BERTSentimentAnalyzer()\n",
        "print([bert_sentime_analyzer.predict_sentiment(text) for text in sample_texts])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v7u5hySxSvZ",
        "outputId": "d7f67076-6add-45c0-ccb1-14cbe7b4df9f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'NEGATIVE', 'confidence': 0.9692}, {'label': 'POSITIVE', 'confidence': 0.9815}, {'label': 'POSITIVE', 'confidence': 0.9694}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üåü Exercise 4: Understanding BERT For Named Entity Recognition (NER)\n",
        "\n",
        "Objective: Explore how BERT identifies entities in text using the NER task.\n",
        "\n",
        "Why this matters:\n",
        "\n",
        "NER helps extract important information like names, locations, and organizations from text. BERT can be fine-tuned for NER using models trained with the B-I-O tagging scheme (Begin, Inside, Outside).\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. Import AutoTokenizer and AutoModelForTokenClassification.\n",
        "2. Create a class BERTNamedEntityRecognizer with methods for:\n",
        "\n",
        "Initializing the tokenizer and model.\n",
        "Recognizing entities in a given text and mapping token predictions to labels.\n",
        "3. Test your recognizer with sample text containing entities.\n",
        "\n",
        "Outcome: You will build an NER system that identifies entities like names, places, and more using BERT."
      ],
      "metadata": {
        "id": "5hVaDHsk2xiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import AutoTokenizer and AutoModelForTokenClassification.\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "#Create a class BERTNamedEntityRecognizer with methods for:\n",
        "class BERTNamedEntityRecognizer:\n",
        "    #Initializing the tokenizer and model\n",
        "    def __init__(self, model_name='dslim/bert-base-NER'):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "    #Recognizing entities in a given text and mapping token predictions to labels.\n",
        "    def recognize_entities(self, text):\n",
        "        self.ner_pipeline = pipeline(\"ner\", model=self.model, tokenizer=self.tokenizer, aggregation_strategy=\"simple\")\n",
        "        return self.ner_pipeline(text)\n",
        "\n",
        "#Test your analyzer with various sample texts.\n",
        "texts = [\n",
        "    \"Barack Obama was born in Hawaii.\",\n",
        "    \"Apple Inc. is based in Cupertino.\",\n",
        "    \"Angela Merkel was Chancellor of Germany.\"\n",
        "]\n",
        "\n",
        "bert_ner = BERTNamedEntityRecognizer()\n",
        "result= [bert_ner.recognize_entities(text) for text in texts]\n",
        "\n",
        "for i, entities in enumerate(result):\n",
        "    print(f\"\\nText {i + 1}:\")\n",
        "    if not entities:\n",
        "        print(\"  No entities found.\")\n",
        "    for entity in entities:\n",
        "        print(f\"  {entity['word']} ({entity['entity_group']}, confidence: {entity['score']:.4f})\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yexk86-2zy7",
        "outputId": "92a71932-558a-4e5a-e78a-357f092713ac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text 1:\n",
            "  Barack Obama (PER, confidence: 0.9993)\n",
            "  Hawaii (LOC, confidence: 0.9997)\n",
            "\n",
            "Text 2:\n",
            "  Apple Inc (ORG, confidence: 0.9994)\n",
            "  Cupertino (LOC, confidence: 0.9977)\n",
            "\n",
            "Text 3:\n",
            "  Angela Merkel (PER, confidence: 0.9982)\n",
            "  Germany (LOC, confidence: 0.9996)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üåü Exercise 5: Comparing BERT And GPT\n",
        "\n",
        "Objective: Understand the architectural and functional differences between BERT and GPT models.\n",
        "\n",
        "Why this matters:\n",
        "\n",
        "BERT and GPT are foundational models in NLP but serve different purposes. Knowing their strengths, weaknesses, and use cases helps you choose the right model for your task.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "1. Research the architectures and applications of BERT and GPT.\n",
        "2. Create a comparison table based on:\n",
        "\n",
        "Architecture (encoder, decoder, or both).\n",
        "Primary purpose (understanding vs. generation).\n",
        "Common use cases.\n",
        "Strengths and weaknesses.\n",
        "3. Reflect on the differences and similarities.\n",
        "\n",
        "Outcome: You will have a clear comparison of BERT and GPT, helping you understand when to use each model."
      ],
      "metadata": {
        "id": "-fOg0SKp9zjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Model Type**                             | **Architecture**               | **Primary Purpose** | **Common Use Cases**                                  | **Strengths**                                                                                           | **Weaknesses**                                                               |\n",
        "| ------------------------------------------ | ------------------------------ | ------------------- | ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |\n",
        "| **BERT** (e.g. `bert-base-uncased`)        | Encoder-only                   | Understanding       | Classification, NER, QA (extractive), embeddings      | - Deep contextual understanding<br>- Bidirectional attention<br>- Great for tasks needing comprehension | - Not designed for generation<br>- Fixed input size                          |\n",
        "| **GPT** (e.g. `gpt-3.5`, `gpt-4`)          | Decoder-only                   | Generation          | Text generation, chatbots, summarization, translation | - Strong generative capabilities<br>- Few-shot/in-context learning<br>- Long-form coherence             | - Not ideal for extractive QA or classification<br>- Needs careful prompting |\n",
        "                                                     |\n"
      ],
      "metadata": {
        "id": "N-HQg3Q5-Rz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üåü Exercise 6: Exploring BERT Applications In Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "Objective: Learn how BERT is used in RAG systems to enhance information retrieval.\n",
        "\n",
        "Why this matters:\n",
        "\n",
        "RAG systems combine retrieval and generation, allowing language models to access external knowledge. BERT plays a key role in retrieving relevant information, improving the quality of generated responses.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "Research the concept of Retrieval-Augmented Generation (RAG).\n",
        "Explain BERT‚Äôs role in the retrieval component.\n",
        "Describe how BERT generates embeddings for documents and queries.\n",
        "Discuss how a vector database is used to match queries with relevant documents.\n",
        "Provide an example of how BERT and a generative model like GPT work together in a RAG system.\n",
        "Outcome: You will understand BERT‚Äôs role in RAG systems and how it enhances retrieval for generation tasks."
      ],
      "metadata": {
        "id": "jSsEdUtl-VAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG is a hybrid architecture that fetches relevant documents or passages from a knowledge base and produces a coherent answer based on retrieved content.\n",
        "This allows large language models (LLMs) to generate grounded, accurate responses using external knowledge.\n",
        "\n",
        "BERT will generate embeddings for both Documents from databases during indexing phase and User queries at runtime. It results in a semantical map of texts in a vector space where distance is a measurement of similarity, even if the wording differs.\n",
        "\n",
        "üìê 3. How BERT Generates Embeddings\n",
        "\n",
        "üîπ For Documents (at indexing time):\n",
        "embedding = bert_model.encode(\"Document text here\")\n",
        "The embedding is a dense vector (e.g., 384 or 768 dimensions).\n",
        "It‚Äôs stored in a vector database for fast lookup.\n",
        "üîπ For Queries (at retrieval time):\n",
        "query_embedding = bert_model.encode(\"What is the capital of France?\")\n",
        "The query is embedded into the same space as the documents.\n",
        "BERT learns to place semantically related inputs (e.g., ‚Äúcapital of France‚Äù and ‚ÄúParis‚Äù) closer together.\n",
        "üì¶ 4. Vector Database for Retrieval\n",
        "\n",
        "A vector database (like FAISS, Pinecone, Weaviate, or Qdrant):\n",
        "\n",
        "Stores all document embeddings\n",
        "Enables fast similarity search (using cosine similarity, dot product, etc.)\n",
        "Returns top-k most relevant documents for a query\n",
        "Example:\n",
        "similar_docs = vector_db.search(query_embedding, top_k=5)\n",
        "These docs are then fed into the generative model.\n",
        "\n",
        "ü§ñ 5. RAG in Action: BERT + GPT Example\n",
        "\n",
        "Let‚Äôs walk through a simplified flow:\n",
        "Step 1: Preprocessing\n",
        "\n",
        "# Document index\n",
        "docs = [\"Paris is the capital of France.\", \"Berlin is the capital of Germany.\"]\n",
        "doc_embeddings = bert_model.encode(docs)\n",
        "vector_db.add(docs, doc_embeddings)\n",
        "Step 2: User asks a question\n",
        "\n",
        "query = \"What is the capital of France?\"\n",
        "query_embedding = bert_model.encode(query)\n",
        "top_docs = vector_db.search(query_embedding, top_k=1)\n",
        "Step 3: Prompt construction\n",
        "\n",
        "prompt = f\"Use the following passage to answer the question:\\n\\n{top_docs[0]}\\n\\nQ: {query}\\nA:\"\n",
        "Step 4: GPT generates an answer\n",
        "\n",
        "answer = gpt_model.generate(prompt)\n",
        "print(answer)  # ‚Üí \"Paris\""
      ],
      "metadata": {
        "id": "V7JHtocxWpt-"
      }
    }
  ]
}