{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJlPexxzLPn9",
        "outputId": "4df92224-8c13-4c6c-b521-5159ec74c282"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Loading and Inspection\n",
        "\n",
        "Load the tennis articles dataset from the .xls file using pandas.\n",
        "Explore the dataset using .head() and .info() to understand its structure.\n",
        "Drop the article_title column to simplify the dataset."
      ],
      "metadata": {
        "id": "3zRjDdRoKXif"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFxhOFwHKPf5",
        "outputId": "1fff530a-da2f-426f-ff2f-6a4658e6e1e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "   article_id                                       article_text  \\\n",
            "0           1  Maria Sharapova has basically no friends as te...   \n",
            "1           2  BASEL, Switzerland (AP)  Roger Federer advanc...   \n",
            "2           3  Roger Federer has revealed that organisers of ...   \n",
            "3           4  Kei Nishikori will try to end his long losing ...   \n",
            "4           5  Federer, 37, first broke through on tour over ...   \n",
            "\n",
            "                                              source  \n",
            "0  https://www.tennisworldusa.org/tennis/news/Mar...  \n",
            "1  http://www.tennis.com/pro-game/2018/10/copil-s...  \n",
            "2  https://scroll.in/field/899938/tennis-roger-fe...  \n",
            "3  http://www.tennis.com/pro-game/2018/10/nishiko...  \n",
            "4  https://www.express.co.uk/sport/tennis/1036101...  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8 entries, 0 to 7\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   article_id    8 non-null      int64 \n",
            " 1   article_text  8 non-null      object\n",
            " 2   source        8 non-null      object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 324.0+ bytes\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (or upload directly)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/tennis_articles.csv\", encoding='latin1')\n",
        "df.drop(labels= [\"article_title\"], axis= 1,inplace= True)\n",
        "print(df.head())\n",
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Sentence Tokenization\n",
        "\n",
        "Use nltk.sent_tokenize() to split the article_text into individual sentences.\n",
        "Flatten the resulting list of sentence lists into a single list of all sentences."
      ],
      "metadata": {
        "id": "hyjCOEcWMtkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfjlRsFkMxVk",
        "outputId": "5c54f63e-9fde-43d6-fec7-a06b62b47d2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "O2aq7GNQNKFR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the punkt tokenizer model\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Apply sent_tokenize to each article text and create a list of lists\n",
        "sentences_lists = df['article_text'].apply(sent_tokenize).tolist()\n",
        "\n",
        "# Flatten the list of lists into a single list of sentences\n",
        "all_sentences = [sentence for sublist in sentences_lists for sentence in sublist]\n",
        "\n",
        "# Example output\n",
        "print(all_sentences[:10])  # print first 10 sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpSTd7agNPMj",
        "outputId": "667dbeec-ab60-4621-fe9f-24d01f30d4b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Maria Sharapova has basically no friends as tennis players on the WTA Tour.', \"The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much.\", 'I think everyone knows this is my job here.', \"When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.\", \"So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\", \"I'm a pretty competitive girl.\", \"I say my hellos, but I'm not sending any players flowers as well.\", \"Uhm, I'm not really friendly or close to many players.\", \"I have not a lot of friends away from the courts.'\", 'When she said she is not really close to a lot of players, is that something strategic that she is doing?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Download and Load GloVe Word Embeddings\n",
        "\n",
        "Download the pre-trained GloVe vectors (e.g., glove.6B.100d.txt).\n",
        "Load the embeddings into a Python dictionary where each word maps to its 100-dimensional vector."
      ],
      "metadata": {
        "id": "BgrUXDweNnRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jT01LJAMOC76"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download GloVe embeddings (if not already downloaded)\n",
        "glove_zip_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "glove_zip_path = \"/content/glove.6B.zip\"\n",
        "glove_folder = \"/content/glove.6B\"\n",
        "\n",
        "if not os.path.exists(glove_zip_path):\n",
        "    print(\"Downloading GloVe embeddings...\")\n",
        "    r = requests.get(glove_zip_url)\n",
        "    with open(glove_zip_path, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "else:\n",
        "    print(\"GloVe zip already downloaded.\")\n",
        "\n",
        "# Step 2: Extract the zip (if not already extracted)\n",
        "if not os.path.exists(glove_folder):\n",
        "    print(\"Extracting GloVe embeddings...\")\n",
        "    with zipfile.ZipFile(glove_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"/content/\")\n",
        "else:\n",
        "    print(\"GloVe folder already extracted.\")\n",
        "\n",
        "# Step 3: Load the 100-dimensional embeddings into a dictionary\n",
        "glove_path = os.path.join(glove_folder, \"/content/glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(glove_path, 'r', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.array(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = vector\n",
        "\n",
        "print(f\"Loaded {len(embeddings_index)} word vectors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrHXh4oPNuQL",
        "outputId": "fb5c4b69-08dc-4003-9388-83187728f571"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe zip already downloaded.\n",
            "Extracting GloVe embeddings...\n",
            "Loaded 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Text Cleaning and Normalization\n",
        "\n",
        "Remove punctuation, special characters, and numbers using regex.\n",
        "Convert all sentences to lowercase to avoid case-sensitive mismatch.\n",
        "Remove stop words using nltk.corpus.stopwords to reduce noise in the data."
      ],
      "metadata": {
        "id": "egtKrPKFOpcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "lSaxxuYnOyfv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords if not already done\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove punctuation, special characters, and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize by splitting on whitespace\n",
        "    words = text.split()\n",
        "    # Remove stop words\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Join back into cleaned sentence\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Example: clean all sentences in your list `all_sentences` from previous step\n",
        "cleaned_sentences = [clean_text(sentence) for sentence in all_sentences]\n",
        "\n",
        "# Show some cleaned sentences\n",
        "print(cleaned_sentences[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB24D9l3OwkU",
        "outputId": "6a59908a-aaca-40f4-86d4-e69b60abafb0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['maria sharapova basically friends tennis players wta tour', 'russian player problems openly speaking recent interview said dont really hide feelings much', 'think everyone knows job', 'im courts im court playing im competitor want beat every single person whether theyre locker room across net', 'im one strike conversation weather know next minutes go try win tennis match', 'im pretty competitive girl', 'say hellos im sending players flowers well', 'uhm im really friendly close many players', 'lot friends away courts', 'said really close lot players something strategic']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Sentence Vectorization\n",
        "\n",
        "For each cleaned sentence:\n",
        "Split into words.\n",
        "Replace each word with its GloVe vector (use a zero-vector if the word is not in the embedding).\n",
        "Compute the average of all word vectors in the sentence.\n",
        "Store all resulting sentence vectors in a list."
      ],
      "metadata": {
        "id": "mUdBAgILQY9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100  # GloVe 100d vectors\n",
        "\n",
        "def sentence_to_vector(sentence, embeddings_index, embedding_dim=100):\n",
        "    words = sentence.split()\n",
        "    if not words:\n",
        "        # Empty sentence => return zero vector\n",
        "        return np.zeros(embedding_dim)\n",
        "\n",
        "    vectors = []\n",
        "    for word in words:\n",
        "        vec = embeddings_index.get(word)\n",
        "        if vec is not None:\n",
        "            vectors.append(vec)\n",
        "        else:\n",
        "            # Word not found in GloVe => use zero vector\n",
        "            vectors.append(np.zeros(embedding_dim))\n",
        "    # Average word vectors\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "# Apply to all cleaned sentences\n",
        "sentence_vectors = [sentence_to_vector(sent, embeddings_index, embedding_dim) for sent in cleaned_sentences]\n",
        "\n",
        "# Example: shape of first sentence vector\n",
        "print(sentence_vectors[0].shape)  # (100,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LLtlvDNQhi-",
        "outputId": "fce167b7-c62c-48c3-b25f-571d9e201339"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Similarity Matrix Construction\n",
        "\n",
        "Initialize an empty matrix of size (number of sentences × number of sentences).\n",
        "Compute pairwise cosine similarity between sentence vectors.\n",
        "Fill in the matrix such that each cell represents the similarity between two sentences."
      ],
      "metadata": {
        "id": "3jwDJIP-RBa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "IbM0JZ_1RPG-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack sentence vectors into a 2D array (if not already)\n",
        "X = np.vstack(sentence_vectors)  # shape (num_sentences, 100)\n",
        "\n",
        "# Compute cosine similarity matrix (num_sentences x num_sentences)\n",
        "similarity_matrix = cosine_similarity(X)\n",
        "\n",
        "print(similarity_matrix.shape)  # should be (num_sentences, num_sentences)\n",
        "\n",
        "# Example: similarity between sentence 0 and sentence 1\n",
        "print(similarity_matrix[0, 1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI5OF9IoRG1-",
        "outputId": "00ea65c5-15b9-4c6b-9341-5ccff25ae27d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(130, 130)\n",
            "0.6426970974554298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Graph Construction and Sentence Ranking\n",
        "\n",
        "Convert the similarity matrix into a graph using networkx.\n",
        "Apply the PageRank algorithm to score the importance of each sentence."
      ],
      "metadata": {
        "id": "GrjyJQbmRb2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity_matrix is your (num_sentences x num_sentences) numpy array\n",
        "\n",
        "# Create a graph from the similarity matrix\n",
        "# We'll use a weighted undirected graph, ignoring self-similarity (diagonal)\n",
        "np.fill_diagonal(similarity_matrix, 0)  # Remove self-loops by zeroing diagonal\n",
        "\n",
        "G = nx.from_numpy_array(similarity_matrix)\n",
        "\n",
        "# Apply PageRank (weights are the edge weights)\n",
        "pagerank_scores = nx.pagerank(G, weight='weight')"
      ],
      "metadata": {
        "id": "UyxYgpSvRf3G"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Summarization\n",
        "\n",
        "Sort all sentences based on their PageRank scores in descending order.\n",
        "Extract the top N sentences (e.g., 10) as the final summary.\n",
        "Print or return the summarized sentences."
      ],
      "metadata": {
        "id": "GvrXN45VSTb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort sentences by PageRank score descending\n",
        "ranked_sentences = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "N = 10  # number of sentences for summary\n",
        "\n",
        "# Extract top N sentence indices (keep their original order if you want a coherent summary)\n",
        "top_sentence_indices = [idx for idx, score in ranked_sentences[:N]]\n",
        "top_sentence_indices.sort()  # optional: sort to keep original order in the text\n",
        "\n",
        "# Get the original sentences (before cleaning, for better readability)\n",
        "summary_sentences = [all_sentences[i] for i in top_sentence_indices]\n",
        "\n",
        "print(\"Summary:\")\n",
        "for sent in summary_sentences:\n",
        "    print(\"-\", sent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuBYo4RBSTDs",
        "outputId": "d4f94d4b-15f8-43f9-f2a8-d2876716023c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "- So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\n",
            "- Speaking at the Swiss Indoors tournament where he will play in Sundays final against Romanian qualifier Marius Copil, the world number three said that given the impossibly short time frame to make a decision, he opted out of any commitment.\n",
            "- Major players feel that a big event in late November combined with one in January before the Australian Open will mean too much tennis and too little rest.\n",
            "- Federer said earlier this month in Shanghai in that his chances of playing the Davis Cup were all but non-existent.\n",
            "- He used his first break point to close out the first set before going up 3-0 in the second and wrapping up the win on his first match point.\n",
            "- I felt like the best weeks that I had to get to know players when I was playing were the Fed Cup weeks or the Olympic weeks, not necessarily during the tournaments.\n",
            "- I just felt like it really kind of changed where people were a little bit, definitely in the '90s, a lot more quiet, into themselves, and then it started to become better. Meanwhile, Federer is hoping he can improve his service game as he hunts his ninth Swiss Indoors title this week.\n",
            "- The former Wimbledon junior champion was full of hope, excited about getting his life back together after a troubled few years and a touch-and-go battle with pancreatitis.\n",
            "- Exhausted after spending half his round deep in the bushes searching for my ball, as well as those of two other golfers hed never met before, our incredibly giving designated driver asked if we didnt mind going straight home after signing off so he could rest up a little before heading to work.\n",
            "- I was on a nice trajectorythen, Reid recalled.If I hadnt got sick, I think I could have started pushing towards the second week at the slams and then who knows. Duringa comeback attempt some five years later, Reid added Bernard Tomic and 2018 US Open Federer slayer John Millman to his list of career scalps.\n"
          ]
        }
      ]
    }
  ]
}