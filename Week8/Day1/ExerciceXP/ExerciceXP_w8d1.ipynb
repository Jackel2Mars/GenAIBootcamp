{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d517aef",
   "metadata": {},
   "source": [
    "Exercise 1: Open Source Levels Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c834c15",
   "metadata": {},
   "source": [
    "1. Gather Definitions\n",
    "\n",
    "First, locate the descriptions of the three openness levels:\n",
    "Copy or highlight each definition in your notes.\n",
    "\n",
    "Fully Open\n",
    "    \"This is the gold standard of openness. Everything is available:\n",
    "        The modelâ€™s architecture (how itâ€™s built)\n",
    "        The weights (what it learned during training)\n",
    "        The training code (how it was trained)\n",
    "        The training data (what it learned from)\n",
    "    This level of transparency means:\n",
    "        Anyone can reproduce the model from scratch.\n",
    "        Researchers can fully understand and audit the model.\n",
    "        Developers can fine-tune or retrain it for their own tasks.\"\n",
    "\n",
    "Weights Released\n",
    "    \"This is a common middle ground. You get the modelâ€™s learned knowledge (its weights), but you donâ€™t get the full picture of how it was trained or on what data.\n",
    "    This still lets you:\n",
    "        Run the model in your own applications.\n",
    "        Fine-tune it on your own dataset.\n",
    "        Deploy it for specific use cases (within licensing limits).\n",
    "    However:\n",
    "        You canâ€™t easily trace back how it was trained.\n",
    "        There may be hidden biases or issues you canâ€™t fully investigate.\"\n",
    "\n",
    "Architecture Only\n",
    "    \"In some cases, only the blueprint of the model is releasedâ€”its structure, layers, and logicâ€”but no weights or training data.\n",
    "    This means:\n",
    "        You can understand the model design.\n",
    "        You could train it yourselfâ€”but youâ€™d need enormous computing power and a large dataset.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e22b02",
   "metadata": {},
   "source": [
    "2. Identify Key Characteristics\n",
    "\n",
    "For each level, write down two bullet points:\n",
    "What is open? (e.g., code, weights, both)\n",
    "What you can and cannot do (e.g., retrain, inspect internals).\n",
    "\n",
    "Fully Open\n",
    "    What is open? Both\n",
    "    What you can do? Run, Retrain, Inspect internals, Write and Modify, Deploy.\n",
    "    What you cannot do? Nothing.\n",
    "\n",
    "Weights Released\n",
    "    What is open? Weights\n",
    "    What you can do? Run, Retrain, Deploy.\n",
    "    What you cannot do? Inspect internals, Write and Modify.\n",
    "\n",
    "Architecture Only:\n",
    "    What is open? Code\n",
    "    What you can do? Run, Retrain (if you have massive amout of data), Inspect internals, Write and Modify, Deploy.\n",
    "    What you cannot do? Nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfddc179",
   "metadata": {},
   "source": [
    "3. Compare Them Side by Side\n",
    "\n",
    "Whatâ€™s Open?\t    Impact on Retraining/Modifying\n",
    "Fully Open\t        You can Run, Retrain, Inspect internals, Write and Modify, Deploy.\n",
    "Weights Released\tYou can Run, Retrain, Deploy. You can't Inspect internals, Write and Modify.\n",
    "Architecture Only\tYou can Run, Retrain (if you have massive amout of data), Inspect internals, \n",
    "                    Write and Modify, Deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b468c7",
   "metadata": {},
   "source": [
    "4. Draft Your Comparative Paragraph\n",
    "\n",
    "While Architecture Only enables you to examine the inner workings of the model, not knowing how it was trained will limit you into what you can do with it, unless you have access to massive data. On the other hand, Weights released won't let you know the inner works, but the kownledge of initial weights enables you to use and retrain with more ease. And of course, Fully Open provides the best of both worlds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f58f5",
   "metadata": {},
   "source": [
    "5. Answer the Healthcare Prompt\n",
    "Since we need to fine-tune the model for a specific topic, we need to at least use a Weight Released model or at best a Fully Open model. Not knowing initial weights, like in Architecture Only disables fine tuning and forces us to actually retrain the model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc58262",
   "metadata": {},
   "source": [
    "Exercise 2: License Check For SaaS Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa68e5",
   "metadata": {},
   "source": [
    "âœ… mistralai/Mistralâ€‘7Bâ€‘Instruct\n",
    "\n",
    "Model page: Hugging Face model card (e.g. mistralai/Mistralâ€‘7Bâ€‘Instructâ€‘v0.3)\n",
    "License section: Apacheâ€¯2.0 \n",
    "Reddit\n",
    "+5\n",
    "Reddit\n",
    "+5\n",
    "Forum d'Alignement de l'IA\n",
    "+5\n",
    "GitHub\n",
    "+12\n",
    "mistralai.github.io\n",
    "+12\n",
    "featherless.ai\n",
    "+12\n",
    "Commercial use explicitly allowed?\n",
    "[x]â€¯Yes â€“ Apacheâ€¯2.0 allows commercial use with attribution and no royalties. \n",
    "Hugging Face\n",
    "mistralai.github.io\n",
    "Restrictions:\n",
    "[ ] Must include Apacheâ€¯2.0 license text and notices when redistributing\n",
    "[ ] Attribution required per Apacheâ€¯2.0 terms\n",
    "[ ] No further commercial/geo limitations listed on Hugging Face\n",
    "âœ… metaâ€‘llama/Llamaâ€‘2â€‘7bâ€‘chatâ€‘hf\n",
    "\n",
    "Model page: Hugging Face model card metaâ€‘llama/Llamaâ€‘2â€‘7bâ€‘chatâ€‘hf\n",
    "License section: Llamaâ€¯2 Community License (sourceâ€‘available), not Apache or OSIâ€‘approved.\n",
    "WikipÃ©dia\n",
    "+5\n",
    "GitHub\n",
    "+5\n",
    "Reddit\n",
    "+5\n",
    "Reddit\n",
    "+10\n",
    "source.kevin125.com\n",
    "+10\n",
    "deepinfra.com\n",
    "+10\n",
    "Reddit\n",
    "+3\n",
    "WikipÃ©dia\n",
    "+3\n",
    "Forum d'Alignement de l'IA\n",
    "+3\n",
    "Commercial use explicitly allowed?\n",
    "[x]â€¯Conditional â€“ permitted for most users, but restricted for certain highâ€‘scale entities.\n",
    "Reddit\n",
    "+10\n",
    "source.kevin125.com\n",
    "+10\n",
    "Reddit\n",
    "+10\n",
    "Restrictions:\n",
    "[ ] Prohibited to use model materials or outputs to improve any other LLM (except derivatives of LlamaÂ 2) \n",
    "Reddit\n",
    "+15\n",
    "source.kevin125.com\n",
    "+15\n",
    "Reddit\n",
    "+15\n",
    "[ ] If your product or service has >700 million monthly active users, you must request a license from Meta; commercial rights not granted until approved. \n",
    "WikipÃ©dia\n",
    "+9\n",
    "source.kevin125.com\n",
    "+9\n",
    "Forum d'Alignement de l'IA\n",
    "+9\n",
    "[ ] Must retain attribution notice (â€œLlama 2 is licensed under the Llama 2 Community Licenseâ€¦â€) when distributing, and provide the license agreement to recipients. \n",
    "source.kevin125.com\n",
    "+1\n",
    "Reddit\n",
    "+1\n",
    "[ ] Must comply with Metaâ€™s Acceptable Use Policy and export/trade regulations\n",
    "source.kevin125.com\n",
    "+1\n",
    "WikipÃ©dia\n",
    "+1\n",
    "ðŸ“ Markdown Checklist Summary\n",
    "- [ ] **Mistralâ€‘7Bâ€‘Instruct (mistralai/Mistralâ€‘7Bâ€‘Instructâ€‘v0.3)**  \n",
    "  - Type of license:  \n",
    "    - [ ] Apacheâ€¯2.0  \n",
    "  - Commercial use allowed:  \n",
    "    - [ ] Yes  \n",
    "  - Restrictions:  \n",
    "    - [ ] Must include license text and notices  \n",
    "    - [ ] Required attribution per Apacheâ€¯2.0\n",
    "\n",
    "- [ ] **Llamaâ€¯2â€‘7Bâ€‘chat (metaâ€‘llama/Llamaâ€‘2â€‘7bâ€‘chatâ€‘hf)**  \n",
    "  - Type of license:  \n",
    "    - [ ] Llamaâ€¯2 Community License (sourceâ€‘available)  \n",
    "  - Commercial use allowed:  \n",
    "    - [ ] Conditional  \n",
    "  - Restrictions:  \n",
    "    - [ ] Cannot use outputs to train/improve other LLMs (outside LlamaÂ 2 derivatives)  \n",
    "    - [ ] Entities with >700M MAU must request a separate Meta license  \n",
    "    - [ ] Must retain attribution notice and include license text in distribution  \n",
    "    - [ ] Must abide by Meta Acceptable Use Policy and export/trade rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49399d4d",
   "metadata": {},
   "source": [
    "ðŸŒŸ Exercise 3: LLM Matchmaker Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66522129",
   "metadata": {},
   "source": [
    "ðŸ§  1. Analyze Team Needs\n",
    "\n",
    "LegalTech: CPU-only inference, logic-heavy chatbot â†’ emphasize logical reasoning, fast CPU inference, compact size.\n",
    "EdTech: Math & logic focus on low-end laptops â†’ need strong math benchmarks, low memory usage, <=â€¯7B params.\n",
    "Global NGO: Must support 5+ languages well â†’ multilingual models with high FLORESâ€‘200 or MMLU multilingual scores.\n",
    "2. Hugging Face Search Filters\n",
    "\n",
    "I used searches like:\n",
    "\n",
    "â€œHugging Face logic reasoning model 7B CPU inferenceâ€\n",
    "â€œHugging Face math benchmark GSM8K MATH 7Bâ€\n",
    "â€œHugging Face multilingual model FLORESâ€‘200 under 7Bâ€\n",
    "I focused on CPUâ€‘compatible or quantized models, tags like logic/math/multilingual, and model size â‰¤â€¯7â€¯B.\n",
    "3 & 4. Candidate Models & Evaluation\n",
    "\n",
    "ðŸ” LegalTech (Logic-heavy, CPU-friendly):\n",
    "Mistralâ€‘7B Instruct: compact, GQA architecture for fast CPU inference, strong reasoning & BoolQ/MMLU performance \n",
    "hugging-face.org\n",
    "+9\n",
    "Reddit\n",
    "+9\n",
    "Reddit\n",
    "+9\n",
    "Reddit\n",
    "Reddit\n",
    "Hugging Face\n",
    "+7\n",
    "chat01.ai\n",
    "+7\n",
    "Reddit\n",
    "+7\n",
    "arXiv\n",
    "+15\n",
    "arXiv\n",
    "+15\n",
    "Reddit\n",
    "+15\n",
    ".\n",
    "SynLogicâ€‘7B (MiniMax): logicâ€‘specialist fineâ€‘tuned on puzzles/Sudoku, +9.5 points over Qwenâ€‘7B on a logic benchmark \n",
    "hugging-face.org\n",
    "+9\n",
    "Medium\n",
    "+9\n",
    "SourceForge\n",
    "+9\n",
    ".\n",
    "SmolLM3â€‘1.7B: very small (â‰ˆ1.7â€¯B), extremely fast on CPU, BoolQ â‰ˆâ€¯85.7% and strong multilingual scores \n",
    "Medium\n",
    "+1\n",
    "Collabnix\n",
    "+1\n",
    ".\n",
    "ðŸ“š EdTech (Math/logic, lowâ€‘end laptop):\n",
    "Mathstralâ€‘7B: expert math model, GSM8K â‰ˆâ€¯77.1%, MATH â‰ˆâ€¯56.6%, quantized GGUF available\n",
    "MathGPT.AI\n",
    "+5\n",
    "Reddit\n",
    "+5\n",
    "Reddit\n",
    "+5\n",
    ".\n",
    "InternLM2â€‘Mathâ€‘7Bâ€‘Plus: outperforms DeepSeekâ€‘Mathâ€‘7B on informal/formal logic benchmarks\n",
    "chat01.ai\n",
    "+7\n",
    "Reddit\n",
    "+7\n",
    "Reddit\n",
    "+7\n",
    ".\n",
    "SmolLM3â€‘1.7B: strong math results (MGSM ~â€¯70.8), tiny footprint for laptops \n",
    "MathGPT.AI\n",
    "+2\n",
    "Medium\n",
    "+2\n",
    "Collabnix\n",
    "+2\n",
    ".\n",
    "ðŸŒ Global NGO (Multilingual):\n",
    "SmolLM3â€‘1.7B: supports many languages, FLORESâ€‘200 â‰ˆâ€¯82.1%, global MMLU â‰ˆâ€¯68.9% \n",
    "arXiv\n",
    "+2\n",
    "Medium\n",
    "+2\n",
    "chat01.ai\n",
    "+2\n",
    ".\n",
    "Gemmaâ€¯3â€‘1B: extremely small, supports 140+ languages, up to 128â€¯k context, runs on minimal RAM (~0.5â€“2â€¯GB) \n",
    "Reddit\n",
    "+5\n",
    "kolosal.ai\n",
    "+5\n",
    "chat01.ai\n",
    "+5\n",
    ".\n",
    "Qwen2.5â€‘Coderâ€‘7Bâ€‘Instruct: supports ~92 languages, huge context, excellent reasoning math/code (HumanEval 88%) \n",
    "Reddit\n",
    "+9\n",
    "Reddit\n",
    "+9\n",
    "Reddit\n",
    "+9\n",
    ".\n",
    "5. Final Picks (Top 2 vs final decision)\n",
    "\n",
    "LegalTech candidates:\n",
    "Mistralâ€‘7B Instruct: fast CPU, strong overall reasoning & architecture optimized for inference.\n",
    "SynLogicâ€‘7B: extremely logic-focused but slightly niche.\n",
    "Pick: Mistralâ€‘7B Instruct â€“ best balance of speed, general reasoning benchmarks, and CPU efficiency.\n",
    "\n",
    "EdTech candidates:\n",
    "Mathstralâ€‘7B: specialized math reasoning with high GSM8K/MATH scores and quantization support.\n",
    "SmolLM3â€‘1.7B: very light and fast, decent math performance but less mathâ€‘optimized.\n",
    "Pick: Mathstralâ€‘7B â€“ stronger in raw math performance, still runs quantized on laptops.\n",
    "\n",
    "Global NGO candidates:\n",
    "SmolLM3â€‘1.7B: outstanding multilingual support and efficient on lowâ€‘end devices.\n",
    "Gemmaâ€¯3â€‘1B: tiniest footprint, excellent language coverage (140+), but less reasoning strong.\n",
    "Pick: SmolLM3â€‘1.7B â€“ best blend of multilingual coverage and reasonable reasoning benchmarks.\n",
    "\n",
    "6. Deliverables\n",
    "\n",
    "ðŸ” Search Filter Summary\n",
    "LegalTech: filters on Hugging Face: CPUâ€‘compatible, logic, math, â‰¤â€¯7B.\n",
    "EdTech: math, logic, CPUâ€‘compatible, GSM8K/MATH tags, â‰¤â€¯7B.\n",
    "Global NGO: multilingual, FLORESâ€‘200, CPUâ€‘compatible, â‰¤â€¯7B.\n",
    "\n",
    "| Team       | Needs                                     | Your Pick                                                         |\n",
    "| ---------- | ----------------------------------------- | ----------------------------------------------------------------- |\n",
    "| LegalTech  | Fast model for logic-heavy chatbot on CPU | **Mistralâ€‘7B Instruct** (fast reasoning, optimized for CPU)       |\n",
    "| EdTech     | Logic/math-focused LLM on low-end laptops | **Mathstralâ€‘7B** (quantized, high GSM8K/MATH performance)         |\n",
    "| Global NGO | Model that speaks 5+ languages well       | **SmolLM3â€‘1.7B** (multilingual FLORESâ€‘200 â‰ˆâ€¯82.1%, efficient CPU) |\n",
    "\n",
    "âœ… Summary\n",
    "Mistralâ€‘7B Instruct wins for LegalTech due to its architectural optimization and strong benchmarks.\n",
    "Mathstralâ€‘7B delivers top-tier math reasoning in lowâ€‘ resource settings, ideal for EdTech.\n",
    "SmolLM3â€‘1.7B excels in multilingual support and CPU efficiency for the Global NGO use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022db9d",
   "metadata": {},
   "source": [
    "ðŸŒŸ Exercise 4: Local Readiness Audit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b46e0",
   "metadata": {},
   "source": [
    "| Requirement               | Your System Specs | Meets Requirement? |\n",
    "|---------------------------|-------------------|--------------------|\n",
    "| RAM (â‰¥ 16 GB)             |        16 GB      | âœ…                  |\n",
    "| Free Disk Space (â‰¥ 40 GB) |      249,13 GB    | âœ…                  |\n",
    "| OS (Linux/WSL2)           |      MacOS 15.5   | âŒ                  |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
